## 大模型微调工具
>大前提：几乎都围绕这几样转

- 底层框架：PyTorch（默认）、JAX 用得少
- 模型库：HuggingFace transformers（加载 LLM / reranker / embedding）
- 参数高效微调：PEFT / LoRA / QLoRA
- 高层训练框架：SentenceTransformers、FlagEmbedding、TRL、LLaMA-Factory、OpenRLHF 等



### 1️⃣ Embedding 模型（语义向量）
#### 常用任务
- 语义检索 / RAG
- 相似度搜索、聚类
- multi-lingual embedding

#### 推荐库/框架

##### （1）SentenceTransformers（强烈推荐）

- 专门为 `embedding + reranker` 训练/推理 做的库
- 优点
    - 自带大量 SOTA 预训练 embedding 模型
    - 训练 loop 封装好了：对比学习、多任务、in-batch negatives 等
    - 和 HF Hub / transformers 原生打通
- 适合：绝大多数自训练或微调 `embedding` 场景

##### （2）FlagEmbedding（BGE 系列）

- 提供 BGE/BGE-M3 等多语种 embedding & reranker 模型 
- 优点：
    - 针对检索、长文本、多语言做了专门优化
    - 自己有 hard negative、训练/评估工具
    - 适合：以 BGE 系列为主、强调检索质量 / 多语言 的项目。

##### （3）直接用 transformers
- 如果你只是：加载一个现成的 embedding 模型 （如 BAAI/bge-large-en）然后算向量，不想用上面这些高层库，直接 transformers 也可以
- 训练 loop 就要自己写了

### 2️⃣ Reranker 模型（重排、cross-encoder）
#### 常用任务
- 检索后对 `TopK` 文档做重排
- 对 `query–document / query–answer` 打分

#### 推荐库/框架

##### （1）SentenceTransformers

- 不只做 embedding，也内置了 CrossEncoder（典型 reranker） API。
- 很适合快速：
    - 加载现成 cross-encoder 模型
    - 用 `pairwise / listwise loss` 做重排训练

##### （2）FlagEmbedding（BGE reranker）
- 官方提供了一系列 `bge-reranker-*` 模型，配套完整使用教程。
- 也是 `cross-encoder` 架构，精度高但计算更重，更适合 `re-rank` `TopK`

##### （3）transformers 自己撸
- 想玩 ColBERT 风格、Late Interaction、模块改动多 → 自己写模型头和训练 loop。

### 3️⃣ SFT（Supervised Fine-Tuning：有监督指令微调）
#### 典型目标

把通用 LLM 变成：问答机器人、代码助手、域内助手

训练格式通常是：(instruction, input) -> output

#### 推荐库/框架（从「工程友好」角度）

##### （1）LLaMA-Factory
- 支持上百种开源 LLM/VLM、集成 SFT、Reward Modeling、PPO、DPO 等，并支持 LoRA / QLoRA 等多种高效微调
- 用法：
    - 基本是写一个 YAML 配置（选模型、LoRA、数据、训练参数）
    - 命令行一跑就行，适合「不想写太多代码」的人
- 适合：从零搞指令微调，想要「一套工具搞定」的小团队/个人。

##### （2）Hugging Face transformers + PEFT

- 传统做法：
    - transformers.Trainer + datasets
    - 加上 PEFT 做 LoRA / QLoRA 微调。
    - 灵活但要自己写 Trainer 配置、数据 collator 等
- 适合：熟 PyTorch / HF 生态，想高度定制

##### （3）TRL
- 虽然 TRL 是为 RLHF 做的，但新版本也有 SFTTrainer / SFT 支持，定位是「统一 post-training 框架」
- 如果你本来就打算后面接 DPO / PPO，那么直接上 TRL 来做 SFT 也挺顺滑。

##### （4）大规模 / 分布式
- 需要多机多卡、大模型：通常会配合 DeepSpeed / FSDP / Accelerate，
LLaMA-Factory & OpenRLHF 这类框架内部已经帮你集成好了

### 4️⃣ RLHF / PPO（经典 RL 版对齐）
#### 场景
- 典型三阶段：SFT → Reward Model → PPO RLHF
- 适合有大量反馈数据 + 大量算力

#### 推荐库/框架

##### （1）Hugging Face TRL（PPOTrainer）

- 官方给了 PPOTrainer，封装了 policy-critic-reward 的流程
- 上手成本相对较低，文档完善。
- 适合：中等规模 RLHF 试验，1–几台机器。

##### （2）OpenRLHF
- 专门的 大规模 RLHF 框架，基于 Ray / vLLM，支持 PPO 等多种算法。
- 优点：
    - 针对 多机多卡、高吞吐 PPO 做了大量工程优化
    - 有 SFT / RM / PPO / DPO 全套流水线
    - 适合：大厂/实验室级别、多节点集群上的 RLHF

##### （3）LLaMA-Factory（PPO）

- 已经把 PPO 集成为一个「stage」，通过配置即可切换
- 对你来说更像一个「命令行开关」，非常省事

### 5️⃣ DPO（Direct Preference Optimization）

DPO 本质上也是 RLHF 家族，但不用显式 reward model，对工程更友好。


#### 推荐库/框架

##### （1）TRL（DPOTrainer）

- 提供了完整 DPOTrainer，包括对 LLM / VLM 的支持。
- 很适合：
    - 你已经做了 SFT
    - 有 preference pairs（chosen / rejected）
    - 想在单机或小规模多卡上做对齐实验

##### （2）LLaMA-Factory（DPO stage）

- 内置 DPO 阶段，直接配 dataset、stage: dpo 即可
- 适合：「从 SFT 到 DPO 全流程都在一个框架里」

##### （3）OpenRLHF
- 也支持 DPO 训练，并且可以跑到多机多卡集群上
- 适合：你已经在用 OpenRLHF 做 PPO/RLHF，希望统一栈