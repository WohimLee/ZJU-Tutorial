## LoRA 微调结果
##### 结论先行

👉 **r=16 相比 r=8：是“稳态小幅提升”，不是质变，但方向完全正确。**
👉 **如果你的目标是“法律推理可用性”，r=16 明显优于 r=8。**
👉 **如果考虑性价比（显存 / 训练成本 / 推理复杂度），r=8 已经是一个非常强的 baseline。**

换句话说：

> **r=8：已经“学会了法律怎么说话”**
> **r=16：开始“更像一个受过法学训练的人”**



### 一、三组模型横向对比（Base / r=8 / r=16）

我把关键指标放在一起，你一眼就能看清趋势。

#### 1️⃣ Closed-book（不给法条）

| 指标                        | Base  | r=8   | r=16      | 趋势解读    |
| ------------------------- | ----- | ----- | --------- | ------- |
| crime_hit_rate            | 0.513 | 0.478 | **0.490** | r=16 回升 |
| law_hit_rate              | 0.340 | 0.541 | **0.581** | 持续提升    |
| range_hit_rate            | 0.110 | 0.195 | **0.201** | 稳定提升    |
| over_spec_rate            | 0.203 | 0.140 | **0.102** | 持续下降    |
| hallucinated_article_rate | 0.517 | 0.531 | **0.560** | 略升（可解释） |

##### 关键解读（Closed-book）

* **法条命中率：0.34 → 0.54 → 0.58**

  * r=16 明显比 r=8 更“敢、也更稳”地给条号
  * 这是法律 SFT 的核心能力之一

* **刑期区间命中：r=8 → r=16 几乎不再涨**

  * 说明：**这个能力在 r=8 已经“学饱和”**
  * 再涨需要的是**数据质量**，不是 rank

* **over_spec_rate 持续下降（20% → 10%）**

  * 这是非常好的信号
  * 说明模型更倾向“规范性表述”，而不是胡给具体年限

* **hallucination 在 closed-book 上升**

  * ⚠️ 但这是**可预期、且不严重的问题**
  * 原因：模型“更确信自己知道法条”，但 closed-book 本身没有约束
  * 这是 **能力提升的副作用，不是退化**



#### 2️⃣ Open-book（给法条）——真正决定成败的地方

| 指标                        | Base  | r=8   | r=16      | 结论      |
| ------------------------- | ----- | ----- | --------- | ------- |
| crime_hit_rate            | 0.281 | 0.502 | **0.556** | 🚀 持续提升 |
| law_hit_rate              | 0.395 | 0.789 | **0.837** | 🚀🚀 极强 |
| range_hit_rate            | 0.084 | 0.299 | **0.319** | 🚀🚀    |
| over_spec_rate            | 0.084 | 0.082 | **0.080** | 稳定      |
| hallucinated_article_rate | 0.782 | 0.374 | **0.333** | 🎯 持续下降 |

##### 这是重点

**Open-book 的表现，已经可以用一句话总结：**

> **r=16 的模型，已经明显“学会如何围绕给定法条进行受约束的法律推理”。**

逐条看：

* **law_hit_rate 83.7%**

  * 这是一个**实用级别**的数字
  * 在真实系统里已经可以接受

* **range_hit_rate 31.9%**

  * 比 r=8 再涨 ~2%
  * 虽然不是飞跃，但说明表达更稳定

* **hallucinated_article_rate 再降（37% → 33%）**

  * 非常重要
  * 说明更高 rank 提升了“引用纪律”



### 二、r=8 vs r=16：工程层面的真实差异

##### ✅ r=8 的本质

* 学会：

  * 法条驱动推理
  * 规范刑期表达
  * 明显减少乱说
* 已经**远超 base**

##### ✅ r=16 额外带来的

* 更高的：

  * 条号一致性
  * crime / law / range 三者的“同时出现概率”
* 更低的：

  * over-spec（乱给具体刑期）
* 更稳定的：

  * Open-book 推理行为

但你也要注意一件事：

> **r=16 的“边际收益”已经明显下降**

这意味着：

* 再上 r=32，大概率性价比会很差
* 提升瓶颈已经不在 rank，而在：

  * 数据结构
  * prompt 约束
  * 评测口径



### 三、如果给一个“工程建议”

#### ✅ 明确建议是：

##### 🎯 **把 r=16 作为你的主模型**

尤其如果你：

* 有 Open-book（法条输入）
* 有检索增强 / 法条检索
* 希望模型“别乱引条号”

这是一个**非常健康、可上线前打磨的状态**。

##### 📌 r=8 的最佳用途

* 资源受限部署
* 或作为对照 / 消融模型
* 或作为蒸馏 teacher 的中间层


### 四、下一步最值得做的 3 件事（比继续调 rank 更重要）

##### 1️⃣ 改评测口径（非常值得）

* `crime_hit`：改成“是否正确归罪”（LLM judge / label set）
* `over_spec`：排除法条刑期
* `hallucination`：open-book 减去 prompt 内条号

👉 这一步做完，你的提升会**看起来更漂亮，也更真实**



##### 2️⃣ 做一个“小型人工 spot-check”

从 `eval_lora/law_openbook.pred.jsonl`：

* 抽 50 条
* 看：

  * 是否围绕给定条文
  * 是否逻辑连贯
  * 是否“像判决书”

你大概率会发现：

> **r=16 已经明显“可用”，而不是 demo。**



##### 3️⃣ 如果你还想继续提升（不靠 rank）

下一步不是再训，而是：

* **prompt 约束**（如“不确定时请说明”）
* **加入 hard negative 样本**（法条相似但不适用）
* **明确输出结构**（罪名 / 条文 / 刑期）

