## 访问服务

### 0) 通用：先确认服务可访问

#### 用 curl 看模型列表（可选）

```bash
curl http://127.0.0.1:8000/v1/models
```

如果能返回 JSON（models 列表），说明服务没问题。

---

### A. 调用 /v1/chat/completions（Chat 接口）

#### A1) Python 脚本：chat_completions.py

```python
#!/usr/bin/env python3
import os
import json
import requests

BASE_URL = os.getenv("BASE_URL", "http://127.0.0.1:8000")
MODEL = os.getenv("MODEL", "Qwen2-7B-Instruct")

url = f"{BASE_URL}/v1/chat/completions"
payload = {
    "model": MODEL,
    "messages": [
        {"role": "system", "content": "你是一个有帮助的助手。"},
        {"role": "user", "content": "用一句话解释 vLLM 的优势。"}
    ],
    "temperature": 0.7,
    "max_tokens": 200,
    "stream": False
}

resp = requests.post(url, json=payload, timeout=120)
resp.raise_for_status()
data = resp.json()

# OpenAI compatible: choices[0].message.content
print(data["choices"][0]["message"]["content"])
```


#### A2) .sh 脚本：chat_completions.sh

```bash
#!/usr/bin/env bash
set -euo pipefail

HOST="${HOST:-127.0.0.1}"
PORT="${PORT:-8000}"
MODEL="${MODEL:-Qwen2-7B-Instruct}"

curl -s "http://${HOST}:${PORT}/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"${MODEL}\",
    \"messages\": [
      {\"role\": \"system\", \"content\": \"你是一个有帮助的助手。\"},
      {\"role\": \"user\", \"content\": \"用一句话解释 vLLM 的优势。\"}
    ],
    \"temperature\": 0.7,
    \"max_tokens\": 200
  }" | python3 -m json.tool
```


### B. 调用 /v1/completions（Prompt 补全接口）

#### B1) Python 脚本：completions.py

```python
#!/usr/bin/env python3
import os
import requests

BASE_URL = os.getenv("BASE_URL", "http://127.0.0.1:8000")
MODEL = os.getenv("MODEL", "Qwen2-7B-Instruct")

url = f"{BASE_URL}/v1/completions"
payload = {
    "model": MODEL,
    "prompt": "vLLM 的核心优势是：",
    "temperature": 0.7,
    "max_tokens": 200,
    "stream": False
}

resp = requests.post(url, json=payload, timeout=120)
resp.raise_for_status()
data = resp.json()

# OpenAI compatible: choices[0].text
print(data["choices"][0]["text"])
```

#### 运行方法

```bash
pip install requests
python3 completions.py
```



#### B2) .sh 脚本：completions.sh

```bash
#!/usr/bin/env bash
set -euo pipefail

HOST="${HOST:-127.0.0.1}"
PORT="${PORT:-8000}"
MODEL="${MODEL:-Qwen2-7B-Instruct}"

curl -s "http://${HOST}:${PORT}/v1/completions" \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"${MODEL}\",
    \"prompt\": \"vLLM 的核心优势是：\",
    \"temperature\": 0.7,
    \"max_tokens\": 200
  }" | python3 -m json.tool
```

