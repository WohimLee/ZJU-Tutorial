## 基础评价指标

### 🟦 1. BLEU（Bilingual Evaluation Understudy）

**常用于：机器翻译、文本生成质量评估**

#### ✔ 核心思想

把模型生成文本与**参考答案**进行比较，看**有多少 n-gram（词序列）匹配上了**。

例如 n-gram：

* 1-gram：词
* 2-gram：词对
* 3-gram：三词组合
  ……

#### ✔ 计算要点

* 统计生成文本中每个 n-gram 有多少出现在参考文本里
* 计算匹配比例（precision）
* 再取多个 n-gram 的几何平均
* 再乘一个**简短惩罚项**（短句可能故意省略难翻的内容）

#### ✔ 直观理解

> BLEU = “我产生的词序列，有多少跟参考答案一样”

#### 👍 优点

* 自动评分
* 可重复
* 在机器翻译评测里历史悠久

#### 👎 缺点

* 过于依赖**字面匹配**
* 同义替换理解不了
  例如：

  > “他来了” vs “他已经到了”
  > 含义一样，但 BLEU 评分可能很低
* 对长文本比较不敏感语义流畅性

---

### 🔴 2. ROUGE（Recall-Oriented Understudy for Gisting Evaluation）

**常用于：文本摘要质量评估**

与 BLEU 相反，ROUGE 更注重：

> **模型输出是否覆盖了参考答案中的重要内容**

#### ✔ 常见版本

* **ROUGE-1**：按词匹配
* **ROUGE-2**：按 2-gram 匹配
* **ROUGE-L**：按最长公共子序列匹配（保留顺序）

#### ✔ 核心指标

* **召回率（Recall）为主**

  ```
  Recall = 匹配词数 / 参考答案中的词数
  ```

但也可以同时计算 Precision 和 F1。

#### 👍 优点

* 适合摘要这种

  * 输出≈参考内容子集
* 捕捉覆盖度

#### 👎 缺点

* 仍然基于词面匹配
* 无法真正理解语义
* 对表达方式多样的文本不友好

---

### 🟡 3. 困惑度 Perplexity（PPL）

**常用于：语言模型评估**

#### ✔ 核心思想

衡量模型对测试语料的“惊讶程度”

> 模型越确信自己预测的词，困惑度越低
> 说明模型越懂这种语言模式

#### ✔ 数学本质

困惑度是交叉熵的指数形式
大致理解为：

```
PPL = 每个词的平均“逆概率”
```

#### ✔ 直观理解

* PPL = 10
  → 模型平均像在从 10 个可能词里挑一个
* PPL = 100
  → 模型更迷茫

#### 👍 优点

* 不需要参考答案（只要语料）
* 是语言模型训练的标准指标

#### 👎 缺点

* **只看概率分布，不看语义**
* 低困惑度 ≠ 文本一定好
* 不适合直接衡量生成质量

例如：

> “猫猫猫猫猫猫猫”
> 模型如果很确定
> → PPL 很低
> → 但显然没什么意义

---

### 🧭 什么时候用哪个？

| 场景      | 推荐指标      | 原因      |
| ------- | --------- | ------- |
| 机器翻译    | BLEU      | 重视字面准确  |
| 文本摘要    | ROUGE     | 看信息覆盖度  |
| 语言模型训练  | PPL       | 衡量预测能力  |
| 文本生成可读性 | 人工评估+辅助指标 | 自动指标仍有限 |

---

### 📌 一句话总结

* **BLEU：我说话像不像参考答案？（偏精确率 Precision）**
* **ROUGE：我有没有覆盖参考内容？（偏召回率 Recall）**
* **PPL：我预测下一个词有多自信？**
