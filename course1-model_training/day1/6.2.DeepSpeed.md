## DeepSpeed
### 一、DeepSpeed 是什么？（一句话定位）

DeepSpeed 是 Microsoft 开源的一个：面向超大模型的分布式训练与内存优化引擎

它解决的不是“怎么改模型”，而是：
- 模型 太大
- 显存 不够
- 训练 太慢 / 扩展性差

📌 本质：训练基础设施层（infra）

### 二、DeepSpeed 主要解决哪些问题？
##### 1️⃣ 显存问题（最核心）
- 参数（Parameters）
- 梯度（Gradients）
- 优化器状态（Optimizer States）

👉 这些在大模型中占 70%+ 显存

##### 2️⃣ 训练效率问题
- 多卡 / 多机扩展
- 通信开销
- 吞吐量

### 三、DeepSpeed 的“杀手锏”：ZeRO
**ZeRO = Zero Redundancy Optimizer**

- 传统数据并行（DDP）的问题：每张卡都有完整模型 + 梯度 + 优化器
- ZeRO 的思想：把这些东西拆开，分散到不同 GPU 上

>ZeRO 的三个阶段（非常重要）

| 阶段         | 优化内容            | 显存节省  |
| ---------- | --------------- | ----- |
| **ZeRO-1** | 切分优化器状态         | ⭐⭐    |
| **ZeRO-2** | 切分梯度 + 优化器      | ⭐⭐⭐   |
| **ZeRO-3** | 切分参数 + 梯度 + 优化器 | ⭐⭐⭐⭐⭐ |

ZeRO-3 的效果
- 单卡只保存 1/N 模型
- 训练 100B+ 参数模型成为可能

📌 代价：通信更频繁、工程复杂度更高

### 四、DeepSpeed 的其它关键能力
##### 1️⃣ Offload（CPU / NVMe）

把参数 / 优化器状态放到：CPU 内存、NVMe SSD、用时间换空间

##### 2️⃣ 混合精度

- FP16 / BF16
- 自动 loss scaling

##### 3️⃣ 通信优化
- AllReduce 优化
- Pipeline 并行
- Tensor 并行（与 Megatron 结合）

### 五、DeepSpeed vs Unsloth

| 维度   | DeepSpeed  | Unsloth    |
| ---- | ---------- | ---------- |
| 核心目标 | **规模化训练**  | **极致单卡效率** |
| 场景   | 多卡 / 多机    | 单卡 / 小规模   |
| 技术路线 | 分布式 + ZeRO | Kernel 优化  |
| 易用性  | 中等偏复杂      | 很简单        |
| 工业级  | ✅          | ⚠️（偏个人/研究） |
