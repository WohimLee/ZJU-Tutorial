## 大模型显存计算

### 一、基础单位与数值精度（所有计算的地基）
#### 1. 存储单位换算（显存一定要用 1024）
```
1 Byte = 8 Bit
1 KB = 1024 Byte
1 MB = 1024 KB
1 GB = 1024 MB
```

⚠️ 显存/内存 = 1024 进制
⚠️ 参数规模常说的 1B = 10⁹（不是 2³⁰）

#### 2. 常见数值精度
| 精度   | Bits | Bytes | 用途       |
| ---- | ---- | ----- | -------- |
| FP32 | 32   | 4     | 优化器、数值稳定 |
| FP16 | 16   | 2     | 主流训练/推理  |
| BF16 | 16   | 2     | 稳定性更好    |
| INT8 | 8    | 1     | 推理       |
| INT4 | 4    | 0.5   | 极限压缩     |

### 二、显存组成总览（先有全局观）

>一次完整训练的显存 =
```
模型参数
+ 梯度
+ 优化器状态
+ 激活值（最大头）
+ 输入输出 Buffer
+ 临时算子 / 通信 Buffer
```

推理时会砍掉 **梯度 + 优化器 + 大量激活**

### 三、输入 / 输出张量显存（最容易被忽略）

```
b: batch size
s: sequence length
h: hidden size / embedding dim / d_model
dtype: FP16
```
Embedding 后输入张量，公式：`Memory = b × s × h × Bytes`


>
```
b = 1
s = 1024
h = 5120
FP16 = 2 Bytes

1 × 1024 × 5120 × 2 ≈ 10 MB
输入 + 输出 ≈ 20 MB
```

✅ 对 13B / 70B 模型来说几乎可以忽略
❌ 但在 **长上下文（32k / 128k）** 会暴涨

### 四、模型参数显存（最稳定、最好算）
#### 1. 参数规模

>LLaMA-13B
```
13B = 13 × 10⁹ 参数
FP16 参数显存
13B × 2 Bytes ≈ 26 GB
```
#### 2. 训练时的「三件套」
| 项目       | 精度   | 显存    |
| -------- | ---- | ----- |
| 模型参数     | FP16 | 26 GB |
| 梯度       | FP16 | 26 GB |
| FP32 主参数 | FP32 | 52 GB |

### 五、优化器显存（训练最大杀手）

以 Adam / AdamW 为例：

#### 1. Adam 的状态
| 状态    | 含义      | 精度   | 显存    |
| ----- | ------- | ---- | ----- |
| m     | 一阶动量    | FP32 | 52 GB |
| v     | 二阶动量    | FP32 | 52 GB |
| param | FP32 参数 | FP32 | 52 GB |

优化器合计 = 156 GB

#### 2. 总训练显存（不含激活）
```
26（参数）+ 26（梯度）+ 156（优化器）= 208 GB
```
- 🚨 这就是为什么单卡永远训不了 13B

### 六、激活值显存（与 batch / seq 强相关）
#### 1. 激活值本质

激活值是 **反向传播需要缓存的中间结果**：
- Q K V
- Attention scores
- FFN 中间层
- LayerNorm 中间结果

2. 你给出的激活公式（非常关键）
Activation Memory ≈
s × b × h × (34 + 5 × a × s / h) × L


其中：

符号	含义
s	seq_len
b	batch_size
h	hidden_size
a	attention heads
L	层数
3. 激活值的工程结论
操作	激活显存
batch ×2	×2
seq ×2	×4（Attention）
层数 ×2	×2

⚠️ 长上下文是激活地狱

七、显存节省技术（训练核心）
1. Gradient Checkpointing（激活重算）
项目	效果
激活显存	↓ 50~70%
计算量	↑ 20~40%

原理：

不存中间激活，反向时重新 forward

2. ZeRO 系列（分布式显存核武器）
ZeRO Stage 对比
Stage	切分对象	显存
ZeRO-1	优化器	↓
ZeRO-2	+ 梯度	↓↓
ZeRO-3	+ 参数	↓↓↓
ZeRO-3 后
单卡显存 ≈ 总显存 / GPU 数

3. CPU / NVMe Offload
Offload	代价
CPU	PCIe 带宽
NVMe	IO 延迟
八、高效微调（PEFT）显存结构
1. LoRA 为例
显存构成变化
项目	全参	LoRA
参数梯度	13B	几百万
优化器	156 GB	< 1 GB
激活	不变	不变

❗ LoRA 不省激活

2. QLoRA（量化 + LoRA）
项目	精度
主模型	INT4
LoRA	FP16
优化器	FP32

→ 24GB 卡可训 65B

九、推理阶段显存（比训练简单）
1. 推理显存公式
模型参数
+ KV Cache
+ IO Buffer

2. KV Cache 显存
KV ≈ 2 × b × s × h × Bytes × L


长对话推理真正的显存杀手

3. 推理优化
技术	效果
INT8 / INT4	参数 ↓
FlashAttention	KV ↓
PagedAttention	KV 分页
Speculative Decode	速度 ↑
十、训练 vs 微调 vs 推理 总对比
阶段	显存压力	主要来源
预训练	⭐⭐⭐⭐⭐	优化器 + 激活
全参微调	⭐⭐⭐⭐	优化器
LoRA	⭐⭐	激活
推理	⭐	KV Cache
十一、工程级显存速算表（经验）
模型	训练（FP16 Adam）	推理（FP16）
7B	~120 GB	~14 GB
13B	~210 GB	~26 GB
70B	>1 TB	~140 GB