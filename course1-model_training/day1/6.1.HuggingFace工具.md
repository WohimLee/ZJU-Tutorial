## Hugging Face

### ä¸€ã€æ•´ä½“ç”Ÿæ€è§†è§’ï¼ˆå…ˆæœ‰ä¸€å¼ â€œåœ°å›¾â€ï¼‰

Hugging Face çš„æ¨¡å‹è®­ç»ƒ / å¾®è°ƒç”Ÿæ€ï¼Œæœ¬è´¨ä¸Šæ˜¯å›´ç»• Transformer å¤§æ¨¡å‹ï¼Œåœ¨ä¸åŒâ€œæŠ½è±¡å±‚çº§â€ä¸Šè§£å†³ä¸åŒé—®é¢˜çš„ä¸€ç»„å·¥å…·ï¼š

| å±‚çº§             | è§£å†³çš„é—®é¢˜                  | ä»£è¡¨åº“                     |
| -------------- | ---------------------- | ----------------------- |
| **æ¨¡å‹ä¸æ¨ç†åŸºç¡€å±‚**   | æ¨¡å‹ç»“æ„ã€Tokenizerã€å‰å‘/åå‘ä¼ æ’­ | `transformers`          |
| **å‚æ•°é«˜æ•ˆå¾®è°ƒå±‚**    | é™ä½æ˜¾å­˜/å‚æ•°é‡æˆæœ¬             | `peft`                  |
| **å¯¹é½ / å¼ºåŒ–å­¦ä¹ å±‚** | SFT / RLHF / DPO / PPO | `trl`                   |
| **æè‡´æ•ˆç‡ä¼˜åŒ–å±‚**    | æ›´å¿«ã€æ›´çœæ˜¾å­˜çš„å¾®è°ƒ             | `unsloth`               |
| **è®­ç»ƒåŸºç¡€è®¾æ–½ï¼ˆè¡¥å……ï¼‰** | åŠ é€Ÿã€åˆ†å¸ƒå¼ã€æ··åˆç²¾åº¦            | `accelerate`, DeepSpeed |

å¯ä»¥ç†è§£ä¸ºï¼štransformers æ˜¯åœ°åŸºï¼Œå…¶å®ƒåº“éƒ½æ˜¯â€œæ’ä»¶æˆ–æ‰©å±•â€

### äºŒã€transformersï¼šä¸€åˆ‡çš„â€œåœ°åŸºâ€
>1ï¸âƒ£ æ ¸å¿ƒåŠŸèƒ½

transformers æ˜¯ Hugging Face ç”Ÿæ€çš„æ ¸å¿ƒåº“ï¼Œæä¾›ï¼š
- é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼ˆLLaMAã€Qwenã€GPTã€BERTã€T5â€¦ï¼‰
- Tokenizer
- æ¨¡å‹ Forward / Backward
- æ ‡å‡† Trainer API
- æ¨¡å‹ä¿å­˜ã€åŠ è½½ã€æ¨ç†

>2ï¸âƒ£ ä½ èƒ½ç”¨ transformers åšä»€ä¹ˆï¼Ÿ

- å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰
- æ¨ç† / éƒ¨ç½²
- ä½œä¸ºå…¶å®ƒåº“çš„åº•å±‚ä¾èµ–

>3ï¸âƒ£ å…³é”®ç‰¹ç‚¹

- æ¨¡å‹å®šä¹‰æƒå¨æ¥æº
- ä¸å…³å¿ƒâ€œä½ æ€ä¹ˆé«˜æ•ˆè®­ç»ƒâ€ï¼Œåªä¿è¯â€œèƒ½è®­ç»ƒâ€

ğŸ“Œ ä¸€å¥è¯æ€»ç»“: transformers = æ¨¡å‹ + è®­ç»ƒå¾ªç¯çš„åŸºç¡€å®ç°

### ä¸‰ã€PEFTï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLow-Rank / Adapter å®¶æ—ï¼‰
>1ï¸âƒ£ PEFT è¦è§£å†³çš„é—®é¢˜

- å¤§æ¨¡å‹å¾®è°ƒçš„é—®é¢˜ï¼š**æ˜¾å­˜ä¸å¤Ÿã€å‚æ•°å¤ªå¤šã€å…¨é‡å¾®è°ƒæˆæœ¬å¤ªé«˜**
- PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰ çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒå¾ˆå°‘çš„ä¸€éƒ¨åˆ†

>2ï¸âƒ£ PEFT æ”¯æŒçš„æŠ€æœ¯
- **LoRA / QLoRAï¼ˆæœ€å¸¸ç”¨ï¼‰ã€AdaLoRAã€Prefix Tuningã€Prompt Tuningã€IAÂ³**
- å…¶ä¸­ï¼š
    - LoRA = å®é™…å·¥ä¸šæ ‡å‡†
    - QLoRA = ä½æ¯”ç‰¹ + LoRA

>3ï¸âƒ£ PEFT å¦‚ä½•ä¸ transformers åä½œï¼Ÿ
- PEFT ä¸è´Ÿè´£è®­ç»ƒå¾ªç¯ï¼Œåªè´Ÿè´£ï¼šæ”¹æ¨¡å‹ç»“æ„ã€ç®¡ç†å¯è®­ç»ƒå‚æ•°ã€ä¿å­˜/åŠ è½½ adapter
- ğŸ“Œ ä¸€å¥è¯æ€»ç»“ï¼špeft = â€œè®©å¤§æ¨¡å‹èƒ½è¢«æ™®é€šäººè®­ç»ƒçš„æŠ€æœ¯é›†åˆâ€
```
transformers æ¨¡å‹
      â†“
peft åŒ…è£…ï¼ˆæ³¨å…¥ LoRA å±‚ï¼‰
      â†“
è¿”å›ä¸€ä¸ªâ€œå¯è®­ç»ƒçš„è½»é‡æ¨¡å‹â€
```



### å››ã€TRLï¼šå¯¹é½ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLHF / DPOï¼‰
>1ï¸âƒ£ TRL çš„å®šä½
- trl = Transformer Reinforcement Learning
- å®ƒè§£å†³çš„æ˜¯ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰ã€äººç±»åå¥½å¯¹é½ï¼ˆRLHFï¼‰ã€åå¥½ä¼˜åŒ–ï¼ˆDPO / IPO / KTOï¼‰


>2ï¸âƒ£ TRL æ”¯æŒçš„è®­ç»ƒèŒƒå¼

| èŒƒå¼             | è¯´æ˜           |
| -------------- | ------------ |
| **SFTTrainer** | ç›‘ç£å¾®è°ƒï¼ˆæŒ‡ä»¤æ•°æ®ï¼‰   |
| **PPOTrainer** | RLHFï¼ˆéœ€è¦å¥–åŠ±æ¨¡å‹ï¼‰ |
| **DPOTrainer** | æ— éœ€å¥–åŠ±æ¨¡å‹çš„åå¥½å­¦ä¹   |
| **ORPO / KTO** | æ–°ä¸€ä»£å¯¹é½ç®—æ³•      |

>3ï¸âƒ£ TRL å’Œ PEFT çš„å…³ç³»
- TRL å‡ ä¹æ€»æ˜¯æ­é… PEFT ä½¿ç”¨
- ä½ å¯ä»¥ç”¨ï¼š
    - transformers + peft + trl
    - unsloth + trlï¼ˆæ›´å¿«ï¼‰
- ğŸ“Œ ä¸€å¥è¯æ€»ç»“: trl = â€œè®©æ¨¡å‹å­¦ä¼šâ€˜äººç±»å–œæ¬¢ä»€ä¹ˆâ€™â€


### äº”ã€Unslothï¼šæè‡´æ•ˆç‡æ´¾ï¼ˆå·¥ç¨‹é»‘ç§‘æŠ€ï¼‰
>1ï¸âƒ£ Unsloth æ˜¯ä»€ä¹ˆï¼Ÿ

- Unsloth æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½å¾®è°ƒæ¡†æ¶ï¼Œç›®æ ‡åªæœ‰ä¸€ä¸ªï¼šåœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šï¼Œè¶…å¿«ã€è¶…çœæ˜¾å­˜åœ°å¾®è°ƒ LLM

>2ï¸âƒ£ å®ƒåšäº†ä»€ä¹ˆï¼Ÿ
- æ‰‹å†™é«˜æ•ˆ CUDA / Triton Kernel
- é‡å†™ attention / backward
- æ·±åº¦æ•´åˆ LoRA / QLoRA
- é’ˆå¯¹ LLaMA / Qwen / Mistral ä¼˜åŒ–

>3ï¸âƒ£ Unsloth ä¸ PEFT / Transformers çš„å…³ç³»
- åº•å±‚ä»ç„¶æ˜¯ transformers
- å†…éƒ¨å…¼å®¹ PEFTï¼ˆLoRAï¼‰
- å¯¹ç”¨æˆ·æ¥è¯´ï¼šå†™æ³•æ›´ç®€å•ã€æ€§èƒ½æå‡å·¨å¤§ï¼ˆ2Ã—ï½5Ã—ï¼‰

>4ï¸âƒ£ ä½¿ç”¨åœºæ™¯
- å•å¡ 24GB / 16GB
- æœ¬åœ° SFT / DPO
- å¿«é€Ÿå®éªŒ / ä¸ªäººå¼€å‘è€…
- ğŸ“Œ ä¸€å¥è¯æ€»ç»“: unsloth = â€œæŠŠ LoRA å¾®è°ƒåšåˆ°ç‰©ç†æé™çš„å·¥ç¨‹å®ç°â€

### å…­ã€å®é™…è®­ç»ƒç»„åˆ
>1ï¸âƒ£ æœ€åŸºç¡€ï¼šå…¨å‚æ•°å¾®è°ƒ
- transformers

>2ï¸âƒ£ å·¥ä¸šæ ‡å‡†ï¼šLoRA å¾®è°ƒ
- transformers + peft

>3ï¸âƒ£ æŒ‡ä»¤å¯¹é½ / RLHF
- transformers + peft + trl

>4ï¸âƒ£ å•å¡æé™æ•ˆç‡
- unsloth (+ trl)