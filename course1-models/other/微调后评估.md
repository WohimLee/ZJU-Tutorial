## 微调后评估
- 同一套评测集：Base / SFT / LoRA / 量化(int8/int4) 都跑同一批样本
- 同一评测方式：固定 prompt、固定解码参数、固定工具/检索配置
- 既看“任务指标”也看“通用回归”：避免“专项涨了、通用掉了”看不出来

### 0. 通用准备：评测设计怎么做才靠谱

>A. 数据集拆分

- Dev：用于反复调 prompt/训练策略（可污染）
- Test：一次性评测（必须“干净”）

每类能力最好 ≥ 200~1000 条（越稳定越好），并按场景分层（简单/中等/困难、短/长、歧义/无歧义）

>B. 三种评测模式（建议都做）
- 离线自动评测：可规模化（指标可复现）
- LLM-as-judge：适合“改写/扩写”等难自动判定任务（但要固定 judge 模型与 rubric）
- 人工抽检：每类随机抽 50~200 条做“最终可信”校验

>C. 统计显著性

- 报告均值 + 置信区间（bootstrap 最常用）
- 关注“掉点”不要看 0.2% 的波动，先看是否超过噪声带

### 1) 如何对比 SFT 后通用能力是否下降（回归测试）

做一套“通用能力回归集”，训练数据里不要出现（或极少出现）。

#### 推荐维度

- 指令遵循：多约束、格式要求、拒答边界
- 推理：简单算术、逻辑、阅读理解
- 写作：摘要、改写、结构化输出
- 多轮对话一致性：记忆/指代/纠错
- 安全与合规：该拒绝的是否还会拒绝

#### 指标

- 选择题/判定题：Accuracy / Macro-F1
- 生成题：用 pairwise win-rate（SFT vs Base）+ 规则打分（如格式/事实一致性）
- 关键：输出“回归清单”：哪些子类下降、下降幅度、失败样例

#### 最有效的做法

让同一个 judge（固定）对 Base 与 SFT 输出做盲评对比（A/B，不告诉哪个是哪个），统计 win/tie/lose。

### 2) 如何验证 function_call 能力有没有提升

把 function calling 拆成 4 个子能力分别评：

- 是否该调用（Call vs No-Call）
- 选对函数（Function name accuracy）
- 参数正确性（JSON schema validity + 字段匹配）
- 多步调用规划（tool-use success rate）

> 评测集构造

- 正例：必须调用工具才能完成（查库存/算费率/查日历）
- 反例：不该调用也能回答的（避免“滥调”）
- 边界例：信息不足时应追问而不是乱填参数

>指标（强烈建议自动化）
- Schema Valid Rate：JSON 可解析 + 满足 schema
- Exact Match：函数名 + 必填参数完全匹配
- Argument F1：字段级 F1（对可选字段更公平）
- End-to-End Success：把工具真的跑起来，看最终任务是否完成（最关键）

>常见陷阱

- 只看“能不能吐出 JSON”，但参数语义错了（比如日期错、城市错）
- 只评单步，不评多步（真实场景经常要多次工具调用）

### 3) 如何验证 意图改写 / 扩写 能力有没有提升

这类“开放式生成”不要只看 BLEU/ROUGE（很容易误判），建议“规则 + judge + 任务效果”三合一。

#### 改写（保持语义）

- 语义一致性：是否保留原意（Judge 打分/对比）
- 约束满足：是否遵守风格/字数/禁词/口吻
- 信息不增不减：是否幻觉新增事实（尤其重要）

#### 扩写（补全细节）

- 覆盖度：是否把需求点写全
- 合理性：新增细节是否自洽、不编造关键事实
- 可执行性：输出是否能直接用于下游（如客服话术、工单）

#### 指标建议

- 规则：长度、格式、敏感词、重复率（n-gram repetition）
- Judge：给 rubric（语义一致/风格/清晰度/不胡编）做 1~5 分
- Pairwise：SFT vs Base 盲比 win-rate（最直观）

### 4) 如何验证 槽位抽取 能力有没有提升

把它当 信息抽取/序列标注 来评，关键是“标准答案要结构化”。

>数据标注

- 定义 schema：intent + slots（slot_name, slot_value, span）
- 处理同义归一（如“明天/12月13日”）
- 多值槽位、缺失槽位、冲突槽位要明确规则

>指标

- Slot F1（推荐）
    - 严格：slot_name + value 完全一致
    - 宽松：value 可做归一化（日期/金额/单位）
- Intent Accuracy
- Joint Accuracy：intent 和所有 slots 都对（最严格，最反映真实效果）

>线上相关指标

- 下游任务成功率（是否能正确触发流程）
- 人工兜底率/转人工率

### 5) 如何验证 text2sql / text2cypher 能力有没有提升

一定要做 可执行评测（execution-based），不要只看字符串相似度。

>评测集

- 每条：自然语言问题 + 数据库 schema + 期望结果（或期望 SQL/Cypher）
- 覆盖：join、group by、子查询、时间过滤、模糊匹配、排序、limit
- 还要覆盖“不可回答/需要澄清”的问题（防瞎编查询）

>指标
- Execution Accuracy（首选）：执行结果是否等价（结果集一致）
- Valid Query Rate：语法可执行比例
- Schema Linking Accuracy：表/字段是否用对（可从 AST 分析）
- Safety：是否生成危险语句（drop/delete/update）—生产必须拦

>Cypher 特有

- 路径匹配是否正确、方向、关系类型、可变长度路径
- 返回字段是否符合预期（nodes/edges/props）

### 6) 如何验证 微调后是否能按一定格式输出

这是“可机检格式遵循”问题，建议完全自动化。

>常见格式类型
- JSON（带 schema）
- YAML
- Markdown 表格
- 固定段落结构（例如：标题/要点/风险/建议）
- 正则模式（如必须包含“结论：”）

>指标
- Parse Rate：能否被解析（JSON/YAML/表格）
- Schema Pass Rate：是否满足 schema（必填字段、类型、枚举）
- Constraint Pass Rate：字数/字段顺序/不出现多余字段/不输出解释文字
- 还有个特别实用的：“修复成本”: 解析失败后，是否能通过一次“请只输出合法 JSON”修复（Recovery Rate）

### 7) 如何验证 LoRA 微调后效果有没有掉点（相对全参/基座）

LoRA 的对比建议分两块：

#### A. 同任务：LoRA vs 全参SFT vs Base

- 在你的核心任务集上：看主指标（F1/ExecAcc/SuccessRate）
- 在通用回归集上：看是否更容易掉（常见：风格变窄、鲁棒性下降）

#### B. 多 LoRA 版本横评（很常见）

- 不同 rank / alpha / target modules
- 看：
    - 主任务收益曲线（rank ↑ 是否收益递减）
    - 回归风险（rank ↑ 是否更像“过拟合指令”）
    - 最终你会得到一个“甜点区”：主任务提升最大且回归最小。

### 8) 如何验证 int4/int8 量化后效果有没有掉点

量化评测要特别注意：掉点通常集中在长上下文、复杂推理、严格格式、数值精度。

>评测建议

- 同一套 Test：Base(FP16/BF16) vs INT8 vs INT4（同 prompt、同采样）
- 分桶统计：
    - 长度桶：<512 / 512-2k / 2k-8k / 8k+
    - 难度桶：简单/中等/困难
    - 输出类型桶：自由生成 vs JSON/SQL

>指标

- 主任务指标（同前）
- 长度敏感指标：长文本下的成功率、格式通过率
- 一致性：同输入多次采样方差是否变大（量化可能更不稳定）

>工程注意
- 不同量化方案（GPTQ/AWQ/bnb/gguf）差异很大，要固定实现与推理参数
- 如果你用 KV cache / flash attention / rope scaling 等，也要固定，不然很难归因

### 一套“最小可用”的评测清单（你可以直接照着落地）

- 通用回归集（500-2000 条）：A/B 盲比 win-rate + 关键子类准确率
- Function-call 集（300-1000 条）：schema pass + 参数 F1 + 端到端成功率
- 改写/扩写集（300-1000 条）：约束规则通过率 + judge rubric + A/B win-rate
- 意图&槽位集（1000+ 条）：intent acc + slot F1 + joint acc
- Text2SQL/Cypher 集（200-1000 条）：可执行正确率 + valid rate + 安全拦截率
- 格式输出集（200-800 条）：parse rate + schema pass + recovery rate
- LoRA/量化对比表：每项指标给出 Δ（相对 FP16 Base / SFT 的差值）+ 置信区间 + 失败样例