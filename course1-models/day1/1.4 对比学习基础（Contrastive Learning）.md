## 对比学习（Contrastive Learning）基础

>本节目标：
- 掌握对比学习的核心思想：拉近正样本、推远负样本
- 知道对比学习在 NLP 中的主要应用：相似度、检索、匹配
- 理解常见的对比学习范式（Siamese、多负样本）
- 理解对比学习与指令微调在任务上的区别

### 📌 一、对比学习的核心思想：拉近正样本、推远负样本

>✨ 核心公式思想
- 给定一个“查询”向量 q：
    - 正样本（positive）：与 q 语义相似
    - 负样本（negative）：与 q 完全不相关


对比学习目标是：
让：

$$
\operatorname{sim}(q, p o s) \uparrow
$$

同时让：
$$
\operatorname{sim}(q, n e g) \downarrow
$$

>常见相似度
- Cosine similarity
- Dot product

>常见损失
- Contrastive Loss
- Triplet Loss
- NT-Xent（SimCLR）

| Loss             | 优点           | 缺点                 | 应用              |
| ---------------- | ------------ | ------------------ | --------------- |
| InfoNCE（NT-Xent） | 效果最优，适合大规模训练 | batch 越大越好（需要 GPU） | SimCSE、CLIP、BGE |
| Triplet Loss     | 对难负样本敏感，效果好  | 数据构造成本高            | DPR、检索模型        |
| Cosine Loss      | 简单易用         | 效果不如前两者            | 小量数据微调          |


### 📌 二、对比学习在 NLP 中的作用

对比学习重点在学习 **高质量文本向量（embeddings）**

#### 1️⃣ 语义相似度学习（Semantic Similarity）

模型能判断两句话是否“意思接近”。

示例：

- “北京今天天气怎么样？”
- “帮我查下北京的天气。”

对比学习能让它们向量接近，适用于：
- 文本重复检测
- 短文本匹配
- 意图识别（Intent Detection）

#### 2️⃣ 在 RAG 中的向量检索（Vector Retrieval）

<div align="center">
    <image src="imgs/rag.png" width=500>
</div>


RAG 的核心是：问句 → embedding → 从向量库中检索相关文档。

对比学习正是训练 embedding 模型的关键方法，使检索更准：
- Qwen3 Embeddings
- BGE（FlagEmbedding）
- M3E


RAG 效果 = 对比学习质量决定。

#### 3️⃣ 文本匹配（Query ↔ Document）

搜索、内容推荐、FAQ 匹配等场景中，需要判断：
- 用户 query 与文档是否相关
- 商品名与描述是否对应
- 问题是否匹配某个答案

对比学习能够学习“跨文本对齐能力”。

### 📌 三、常见对比学习范式
#### 1️⃣ Siamese / 双塔结构（Siamese Networks）
<div align="center">
    <image src="imgs/siamese.png" width=500>
</div>

这是 NLP 中最常见的结构：
```
query_encoder(q) ——→  向量 q
doc_encoder(d) ——→    向量 d
```

- 两个 encoder 参数相同（共享权重）
- 用 cosine/dot 相似度计算匹配度
- 推理阶段可离线计算 doc embedding → 速度极快

优点：
- 可扩展性强
- 适用于大规模检索（如百万级向量库）

代表模型：
BGE、SimCSE、DPR、E5 等

#### 2️⃣ In-Batch Negative（批量负样本）

在一个 batch 内的其他样本视为“负样本”。

如果 batch size = 128
每个 query 有 127 个负样本 → 自然构成 “大量负样本”。

优点：
- 无需人工构建负样本
- GPU 利用率高
- 效果好

SimCLR、SimCSE、CLIP 都用这种方式。

#### 3️⃣ Hard Negative（难负样本）
什么是难负样本？

错误率高、模型容易混淆的负样本，如：
- Query：“收益率怎么算？”
- Hard negative：“利率怎么算？”

比随机负样本“更难”，训练效果更好。

优点：

- 训练更“有挑战性”
- 显著提升检索质量
- 是向量模型训练的关键（特别是 BGE）

### 📌 四、对比学习的数据

#### 1️⃣ 如何打标

>Pair 形式（正样本标注）
- 本质是：query ↔ positive 的配对关系。
- 无需“负标签”，因为 negatives 来自：
    - in-batch neg
    - 随机负
    - 硬负（hard neg）

>Triplet 形式
- 隐含标签：
    - positive → 1（更相似）
    - negative → 0（不相似）
- 但在实际训练中，不会显式写成 0/1，而是通过 loss 自动构造。

>Hard Negative 的标注方式
- 数据中示例：
    ```json
    {
    "query": "如何提高收益率？",
    "positive": "...正确解释收益率计算...",
    "hard_negative": "...与“利率”相关，但不属于收益率..."
    }
    ```
- Hard negative 的语义接近但不一致
→ 非常有助于提升模型检索能力。

#### 2️⃣ Loss 函数
>NT-Xent Loss（最常用，对应 SimCLR/SimCSE）

$$
L=-\log \frac{\exp (\operatorname{sim}(q, p o s) / \tau)}{\sum_j \exp \left(\operatorname{sim}\left(q, n e g_j\right) / \tau\right)}
$$
- 核心思想：
    - 拉近 q 与 pos
    - 推远 q 与所有 neg（包含 in-batch neg）
- 特点：
    - 效果最稳
    - 广泛用于 embedding、检索模型

>Triplet Loss（三元组损失）

$$
L=\max (0, m+\operatorname{sim}(q, n e g)-\operatorname{sim}(q, p o s))
$$
- 其中 m 是 margin。

- 含义：
    - 如果 positive 还不够接近
    - 或 negative 不够远
    → 就产生 loss 进行更新
- 优点：
    - 可控性强
    - 推荐系统、检索系统常用

>CosineSimilarity / MarginRanking Loss

- 常用于简单任务：
    $$
    L=1-\operatorname{sim}(q, p o s)
    $$
- 或：
    $$
    L=\max (0, m-\operatorname{sim}(q, p o s)+\operatorname{sim}(q, n e g))
    $$
- 适合：
    - 小数据集
    - 微调 embedding
    - 简单语义匹配

### 📌 五、对比学习 vs 指令微调：何时用什么？

这是很多团队容易搞混的点！

下面从任务、目标、模型、效果 4 个维度对比：

##### 1️⃣ 训练目标不同
| 方法            | 训练目标           | 模型能力     |
| ------------- | -------------- | -------- |
| **对比学习**      | 学习**向量语义空间**   | 擅长相似度/检索 |
| **指令微调（SFT）** | 学习**指令到回答的映射** | 擅长生成/对话  |

##### 2️⃣ 模型类型不同
- 对比学习 → Embedding 模型（Dual Encoder）
- 指令微调 → Chat/Generative 模型（Decoder）

##### 3️⃣ 适用场景差异
适合对比学习：

- 🌟 RAG / 检索系统
- 🌟 FAQ 匹配
- 🌟 文档查找、相似度计算
- 🌟 多模态匹配（如 CLIP）

适合指令微调：

- 🌟 问答
- 🌟 解释型、生成型任务
- 🌟 对话助手
- 🌟 指令遵从能力提升

##### 4️⃣ 性能差异总结
| 能力       | 对比学习       | 指令微调       |
| -------- | ---------- | ---------- |
| 向量检索     | ⭐⭐⭐⭐⭐      | ⭐⭐         |
| 文本匹配     | ⭐⭐⭐⭐⭐      | ⭐⭐⭐        |
| 对话生成     | ⭐          | ⭐⭐⭐⭐⭐      |
| 长文本生成    | ⭐          | ⭐⭐⭐⭐       |
| RAG 组件定位 | **检索模块核心** | **生成模块核心** |



### 📌 六、本节小结（Summary）

- 对比学习通过“正样本更近、负样本更远”学习语义空间
- 在 NLP 中被广泛应用于：相似度、检索、匹配
- 主流范式包括：双塔结构、In-batch negative、Hard negative
- 对比学习 ≠ 指令微调
    - → 一个做“检索/匹配”
    - → 一个做“生成/对话”
- 在 RAG 系统中，两者协同构成完整闭环