## 课程大纲：Day 1 —— 大模型训练、微调基础
>课程目标

- 理解大模型训练全流程、PEFT、LoRA/QLoRA
- 掌握对比学习的核心思想及其在语义向量学习中的价值
- 理解并使用 FlagEmbedding（如 BGE、Jina Embedding 等）
- 完成一次微调任务（分类/指令微调/向量模型微调）
- 掌握 GUI 或 Python 两种操作方式


### 1. 理论模块（Theory）
#### 1.1 大模型训练与微调流程
- 数据准备 → 训练 → 评估 → 部署的标准流程
- 预训练与微调的区别
- 模型对齐（Alignment）与指令微调（Instruction Tuning）简介

#### 1.2 迁移学习与 PEFT（参数高效微调）
- 为什么需要 PEFT：成本、显存、工程效率
- 常见 PEFT 方法：LoRA、QLoRA、Adapter、Prefix Tuning 等

#### 1.3 LoRA / QLoRA 原理与适用场景

- LoRA 的低秩分解思想
- QLoRA 的 4-bit 量化机制
- 两者的优缺点与典型使用场景

#### 1.4 对比学习基础（Contrastive Learning）

- 对比学习的核心思想：拉近正样本，推远负样本
- 在 NLP 中的作用：
    - 语义相似度学习
    - 向量检索（RAG）
    - 文本匹配（如 query ↔ document）
- 常见范式：
    - Siamese / 双塔结构
    - In-batch neg / hard negative
- 对比学习 vs 指令微调：适用场景差异、性能对比


### 2. 案例讲解（Case Study）
#### 2.1 LoRA 微调案例

基于 LLaMA-7B 或 Qwen-7B 做分类或指令微调。

#### 2.2 对比学习 + FlagEmbedding 实战案例

- 使用 BGE 进行语义检索向量训练
- 训练数据格式：`query, positive passage, negative passage`
- 对比学习损失查看与可视化（margin loss / cosine similarity）
- 效果评估方式：
    - 检索命中率（Recall@k）
    - 向量可视化（TSNE）

#### 2.3 全参数 vs LoRA vs 对比学习向量微调对比

- 全参数微调：适用于生成任务
- LoRA 微调：适合指令微调
- 对比学习微调：适合检索场景
- 三者的显存、数据量、效果各自优势

### 3 实操环节（Hands-on Lab）
#### 3.1 无 Python 基础学员路径
>工具：可视化/低代码平台
- 如 ModelArts、魔塔、ModelScope Studio 等

>实操内容
- 上传训练数据
- 选择预训练模型与微调方法
- 设置 LoRA 配置（r、alpha、dropout 等）
- 配置训练参数（epochs、batch size、lr 等）
- 启动训练任务并查看日志/结果

#### 3.2 有 Python 基础学员
##### A. LoRA 微调（同前）
>工具
- LlamaFactory / transformers / peft
>实操内容
- 数据集准备
    - 加载 JSON/CSV 数据
    - 文本清洗、格式转化、数据划分
- LoRA 配置
    - 选择目标层
    - 配置 LoRA 超参数
    - QLoRA 的量化参数（4bit quantization config）
- 训练脚本编写
    - 使用 transformers 的 Trainer
    - 使用 LlamaFactory 的快速启动方式
    - peft 的 LoRA 模块注入方式
- 训练与调试
    - 启动训练
    - 查看 loss、learning rate、eval metric 等日志
    - 保存与加载微调后模型



##### B.FlagEmbedding 对比学习训练实操
>工具
- 使用 FlagEmbedding / sentence-transformers 进行向量模型训练：
>实操内容
- 数据加载
    - JSONL 格式：`{"query": "", "pos": "", "neg": ""}`
    - 或 `三元组 (triplet)` 数据
- 构建对比学习数据集
    ```py
    from FlagEmbedding import FlagModel
    ```
- 定义模型与损失函数
    - CosineSimilarityLoss
    - ContrastiveLoss
    - RankingLoss
- 训练脚本示例关键模块
    - 模型加载（如 BGE-base）
    - 训练参数设置（batch size、negatives、lr 等）
    - 训练循环
- 效果验证
    - 计算向量相似度
    - 输出 eval 指标（Recall@k、MRR）

### 4 Day 1 交付作业
#### 无 Python 基础组（任选其一）
- 提交可视化平台训练任务截图
- 包含模型选择
- LoRA 参数
- 训练结果/日志截图

#### Python 组（三选一）
##### ① LoRA 微调脚本

提交完整可运行的 LoRA 微调脚本（`.py` 或 `.ipynb`）
内容需要包含：

- 数据加载与处理
- LoRA 配置代码
- 模型微调训练脚本
- 附上训练日志或模型输出截图

##### ② FlagEmbedding 对比学习训练脚本

- 数据加载
- 对比学习损失
- 模型训练代码
- 示例测试输出

##### ③ 混合任务（可选）
- LoRA 指令微调 + 向量模型训练
- 展示两类模型在 RAG 中的不同作用

### 5 课后预期成果（增强版）
- 掌握生成模型 + 检索模型的基础训练方法
- 完成至少一种 LoRA 或向量模型的微调
- 理解 RAG 系统中“生成模型 + Embedding 模型”的协同关系
- 为 Day2 的 RAG / Agent / 检索增强任务打下基础