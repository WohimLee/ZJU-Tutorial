
>工具：可视化 / 低代码平台 —— LLaMA-Factory UI

<div align=center>
    <image src="imgs/llama_webui.png" width=800>
</div>

本环节让零代码学员也能在 15–30 分钟内完成一次大模型 LoRA 微调。

📌 实操流程概览

- 学员只需要通过 Web UI 完成 5 个步骤：
    - 上传训练数据
    - 选择预训练模型（如 LLaMA-7B 或 Qwen-7B）
    - 选择微调方式（LoRA / QLoRA）
    - 配置训练参数
    - 启动训练并查看结果
- 整个过程无需任何编程基础。

### ① 上传训练数据（Upload Dataset）

支持格式（常见）：

- SFT 格式 JSONL（alpaca / sharegpt）
- 分类任务格式（text + label）
- 问答/对话格式

示例格式（Alpaca 风格）：
```py
{
  "instruction": "总结下面的内容。",
  "input": "……原文……",
  "output": "……总结……"
}
```

操作步骤：
- 打开 LLaMA-Factory UI
- 进入「Dataset」
- 点击 Upload 上传文件
- 系统自动校验格式，并显示样例记录

### ② 选择预训练模型与微调方法

可选模型：
- LLaMA-2 / 3 系列（7B / 13B）
- Qwen3 / Qwen2 / Qwen1.5 系列


选择微调方式：
- LoRA（默认推荐）
- QLoRA（显存更低）

操作步骤：
- 进入「Model」
- 下拉选择你想微调的基础模型
- 选择微调方法：LoRA / QLoRA

### ③ 设置 LoRA 配置（核心模块）


需要设置的主要参数：

| 参数                 | 含义         | 推荐值（入门）              |
| ------------------ | ---------- | -------------------- |
| **r**              | 低秩分解维度     | 8 / 16               |
| **alpha**          | 缩放因子       | 16 / 32              |
| **dropout**        | 防止过拟合      | 0.05 / 0.1           |
| **target_modules** | 注入 LoRA 的层 | 默认即可（q_proj, v_proj） |


>操作步骤

- 进入「LoRA Configuration」
- 设置 r、alpha、dropout
- 其余保持默认即可（确保新手能顺利训练）

>课堂提示
- → r 不宜过大，否则显存上升
- → dropout 可根据数据规模调节

### ④ 配置训练参数（Training Arguments）


常用参数说明：

| 参数                 | 含义      | 新手建议        |
| ------------------ | ------- | ----------- |
| **epochs**         | 全量训练几轮  | 3–5         |
| **batch size**     | 每次训练样本数 | 2–8（受显存影响）  |
| **learning rate**  | 学习率     | 2e-4 或 1e-4 |
| **max seq length** | 最大长度    | 1024–2048   |
| **warmup ratio**   | 预热比例    | 0.1         |


操作步骤：
- 点击「Training」选项卡
- 输入上表中的参数
- 显存不足可调小 batch size

课堂提示：
→ batch size 受显存影响最大
→ 学习率不宜过大，避免模型不稳定

### ⑤ 启动训练任务并查看日志 / 结果


训练界面会显示：
- 当前 epoch / step
- loss 曲线
- GPU 显存占用
- 训练速度（samples/sec）
- 是否发生梯度爆炸 / nan

学员任务：
- 观察 loss 是否持续下降
- 确认 GPU 占用是否稳定
- 最终查看「Checkpoint」生成是否成功

输出内容包含：
- LoRA Adapter 权重（若是 QLoRA，则有 4bit 权重 + LoRA）
- 训练日志
- 可在 Inference 页面测试的微调后模型

### 🎯 本节小结
| 步骤      | 学员能学到什么               |
| ------- | --------------------- |
| 上传数据    | 理解数据格式的重要性            |
| 选择模型    | 掌握选型思路（LLaMA / Qwen）  |
| 设置 LoRA | 理解参数高效微调的核心原理         |
| 设置训练参数  | 掌握微调稳定性的关键因素          |
| 查看训练结果  | 理解训练过程 & debugging 方法 |


本节的目标是让无 Python 基础的学员 可以独立完成一次 LoRA 微调任务，为 Day2 的 RAG / Agent / 部署环节打下基础。