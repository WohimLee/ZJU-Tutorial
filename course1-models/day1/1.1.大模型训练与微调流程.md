## 大模型训练与微调流程
⭐ 部分标题页：大模型训练与微调流程

>本节目标
- 理解大模型从数据 → 训练 → 评估 → 部署的标准流程
- 理解预训练与微调的差异
- 掌握模型对齐（Alignment）及指令微调的作用

### 📌 一、LLM 全流程概览：数据 → 训练 → 评估 → 部署
<div align="center">
    <image src="imgs/llm_training_pipeline.png" width=>
</div>

#### 1）数据准备（Data Preparation）

大模型的训练效果高度依赖数据质量。
典型数据准备包括：

- 数据收集：网络爬虫、开源语料、企业私有数据
- 数据清洗：去噪、去重、文本规范化
- 数据标注：指令数据、对话数据、分类标签等
- 数据格式化：转换成模型可读格式（如 JSONL、HF Datasets）

🔎 关键点：高质量数据比模型规模更重要！

#### 2）模型训练（Training）

训练包括两种不同阶段：

##### ● 预训练（Pre-training）

- 使用海量未标注数据（如网页、书籍、代码）
- 模型任务通常是自监督（如预测下一个 token）
- 目标：学习通用语言能力
- 成本高、数据需求极大，一般由大厂/科研机构完成

##### ● 微调（Fine-tuning）

- 使用任务相关的、规模较小的标注数据
- 常见任务：问答、分类、摘要、对话
- 可以使用 PEFT / LoRA 等低成本方式进行
- 目标：使模型“适应特定任务或领域”

##### 📘 区别总结：
| 项目   | 预训练      | 微调         |
| ---- | -------- | ---------- |
| 数据规模 | 海量（TB 级） | 小规模（MB–GB） |
| 数据类型 | 未标注      | 标注或成对数据    |
| 目标   | 学习语言基础能力 | 让模型更懂任务    |
| 成本   | 极高       | 可在单卡/小集群完成 |
| 谁来做  | 大厂/大模型公司 | 普通开发者、企业   |

#### 3）模型评估（Evaluation）

评估方式因任务而异：

##### ● 自动评估指标

- 分类准确率、F1、AUC
- Rouge、BLEU（生成任务）
- MMLU、CMMLU（通用能力）
- Retrieval recall@k（检索任务）

##### ● 人类评估（Human Evaluation）

- 对话流畅度
- 事实正确性
- 安全性与价值偏好

##### ● 对齐任务的专用评估

- 审查是否“有害输出减少”
- 是否遵循指令
- 是否符合人类偏好

#### 4）模型部署（Deployment）

部署方式包括：
- 本地推理（CPU/GPU）
- 云服务（如 ModelArts、SageMaker、阿里 PAIE）
- 量化后部署（INT8/INT4/FP8）以节省显存
- 在线服务（API）或离线批处理（Batch）

关键问题：
- QPS（吞吐）
- 延迟（Latency）
- 显存与成本
- 安全与隐私

### 📌 二、预训练 vs 微调：为什么不能只靠预训练？

预训练的大模型往往：

- 语言能力强
- 泛化能力好
- 但缺乏任务意识（task awareness）
- 不知道“你希望它做什么”

因此需要：

- 指令微调（Instruction Tuning）
- 对齐（Alignment）
- 强化学习（RLHF）

使模型更像“助理”，而不是“语言生成器”。

### 📌 三、模型对齐（Alignment）与指令微调（Instruction Tuning）
#### 1）为什么需要对齐？

未经对齐的大模型可能：
- 输出不安全内容
- 不遵守用户指令
- 编造事实（hallucination）
- 风格不一致

对齐的目标：让模型行为更聪明、更可控、更符合人类价值与需求。

#### 2）指令微调（Instruction Tuning）是什么？

使用大量 (指令，回答) 的数据，教模型“如何执行任务”。

例如：
```
指令：总结下面一段文本。
文本：……
回答：……
```

作用：
- 增强“遵从指令”的能力
- 提高模型可用性
- 通过数据对齐模型行为，而不是手写规则

常见数据集：

- Alpaca、ShareGPT、OpenOrca、UltraChat
- 企业内部任务数据（问答、客服、知识库）

#### 3）对齐三阶段（业界常用）
##### ① SFT（Supervised Fine-tuning）

监督微调：用高质量回答示范（Demonstration）

##### ② RLHF（Reinforcement Learning from Human Feedback）

用人为偏好训练 Reward Model，再做策略优化

##### ③ DPO、ORPO、PPO 等更高效的对齐技术

以更低成本实现“偏好对齐”

### 📌 四、本节小结（Summary）
LLM 的完整流程是：
- 数据 → 预训练/微调 → 对齐 → 评估 → 部署
- 预训练解决“语言能力”，微调解决“任务能力”
- 指令微调是让模型“变成助手”的关键步骤
- 对齐（Alignment）确保模型“有用且安全”