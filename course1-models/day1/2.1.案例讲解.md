## 案例讲解
### 1 LoRA 微调案例：基于 Qwen3-8B 的分类或指令微调
>📌 案例目标
- 使用 LoRA 对开源模型（如 LLaMA-7B 或 Qwen-7B）完成一次小型任务微调：
    - 文本分类（Sentiment / Intent 分类）
    - 指令微调（Instruction Tuning：问答、改写、总结）

#### 📘 Step 1：准备数据集

两种常见格式：

>① 文本分类格式
```py
{"text": "服务态度很好，下次还会光顾。", "label": "positive"}
```
>② 指令微调格式（Alpaca 风格）
```py
{
  "instruction": "总结下面的内容",
  "input": "……原文……",
  "output": "……总结……"
}
```
#### 📘 Step 2：配置 LoRA 结构

常用参数：

| 参数             | 含义                       |
| -------------- | ------------------------ |
| r              | 低秩分解维度（一般 8–64）          |
| alpha          | 缩放因子                     |
| dropout        | 防止过拟合                    |
| target_modules | 对 Q/K/V 或 linear 层插 LoRA |


LoRA 仅训练极少量参数，因此在 7B 模型上也可轻松训练。

#### 📘 Step 3：启动训练（以 transformers/peft 或 LlamaFactory 为例）

>流程

- 冻结原模型权重
- 注入 LoRA Adapter
- 用监督微调（SFT）方式训练
- 记录 loss / eval 指标

>训练时 GPU 占用：
- Qwen-8B：约 14–20GB（取决于 batch size 和 bf16/fp16）

#### 📘 Step 4：模型效果展示（示例）

任务：客服问答指令微调

输入：
```
请解释退款流程
```

微调前输出（偏离主题、啰嗦）
微调后输出（简洁、准确）：
```
您可在订单详情页点击“申请退款”，系统将引导您选择原因并提交申请。
```

说明：
👉 LoRA 非常适合让模型学到 **业务流程、客服问答、专业知识语气** 等风格。

### 2.2 对比学习 + PEFT 实战案例（基于 BGE 向量模型）

>📌 案例目标
- 使用 BGE 模型进行：
    - 语义检索（semantic search）向量训练
    - 训练 loss 可视化（margin loss / cosine similarity）
    - 检索能力评估（Recall@k）
    - 向量空间可视化（TSNE）

#### 📘 Step 1：准备对比学习数据（Triplet 格式）

>数据格式
```
{
  "query": "怎么办理退货？",
  "positive": "您可以在订单详情页点击申请退货。",
  "negative": "如何修改登录密码？"
}
```

>意义
- positive：与 query 强语义关联
- negative：与 query 不相关
- 一批对比样本构成大量 in-batch negatives

#### 📘 Step 2：选择模型：BGE-small/base/large

>BGE 特点：
- 专为检索优化
- 对比学习训练
- 多任务、多语言支持
- 向量质量非常高
- 可与 LoRA/PEFT 结合完成轻量级训练（参数少、显存低）

#### 📘 Step 3：训练与 Loss 可视化

常用 Loss：Margin Ranking Loss / CosineSimilarity Loss

Margin Loss 曲线示例：

（放折线图）

意义：
- 趋势下降 → 模型逐渐把正样本拉近、负样本推远
- 稳定收敛说明 embedding 空间结构越来越清晰

#### 📘 Step 4：检索效果评估（Recall@k）

Recall@k：检索的相关性命中率

| k  | Recall@k 意味着…  |
| -- | -------------- |
| 1  | Top1 是否命中正确文档  |
| 5  | 前5个文档里是否包含正确文档 |
| 10 | 前10是否命中        |


RAG 系统通常要：
- Recall@5 ≥ 0.7
- Recall@10 ≥ 0.85

越高说明检索准确度越强。

#### 📘 Step 5：向量可视化（TSNE）

可视化结果呈现：
- 相似 query/文档聚成簇
- 不同主题分布明显分离
- 微调后类簇更清晰、距离结构更合理

说明：
👉 对比学习训练能够显著提升 embedding 空间质量。

### 2.3 全参数 vs LoRA vs 对比学习微调：三者对比总结
📌 对比维度：任务类型 / 显存 / 数据量 / 效果
>① 适用任务

| 微调方式        | 擅长任务                 |
| ----------- | -------------------- |
| **全参数微调**   | 生成任务、高性能对话、复杂推理      |
| **LoRA 微调** | 指令遵循、问答、领域知识注入       |
| **对比学习微调**  | 语义检索、RAG、文本匹配、向量空间建立 |

>② 显存需求（以 7B 模型为例）

| 微调方式           | 显存                |
| -------------- | ----------------- |
| 全参数微调          | 40GB+             |
| LoRA           | 12–20GB           |
| 对比学习（BGE Base） | 6–12GB（emb model） |


👉 对比学习的 embedding 模型通常比 LLM 小很多，因此非常高效。

>③ 数据需求

| 微调方式      | 数据规模                    |
| --------- | ----------------------- |
| 全参数微调     | 10k–1000k+（大量示例）        |
| LoRA 指令微调 | 1k–50k（高质量 SFT 数据）      |
| 对比学习      | 5k–50k（pair/triplet 数据） |


>④ 效果特点总结

| 能力   | 全参    | LoRA  | 对比学习  |
| ---- | ----- | ----- | ----- |
| 生成能力 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐  | ⭐     |
| 指令遵循 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐     |
| 检索能力 | ⭐⭐    | ⭐⭐⭐   | ⭐⭐⭐⭐⭐ |
| 训练成本 | 极高    | 低     | 极低    |
| 部署难度 | 高     | 中     | 低     |

📌 关键结论

- 要提升“对话与生成” → 用 LoRA / 全参
- 要提升“检索能力与语义相似度” → 用对比学习模型（BGE/Qwen Embedding）
- RAG 系统需要两者协同：检索用对比学习，回答用 LoRA 微调大模型

🎉 本案例部分总结
| 案例                | 学到什么                     |
| ----------------- | ------------------------ |
| **LoRA 指令微调案例**   | 如何用小显存高效提升 LLM 的领域能力     |
| **对比学习 + BGE 实战** | 如何训练高质量向量检索模型            |
| **三者对比**          | 何时使用 LoRA，何时使用全参，何时用对比学习 |
