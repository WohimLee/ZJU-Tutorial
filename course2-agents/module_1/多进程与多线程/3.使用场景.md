
## 使用场景

### 为什么有的函数加了 async，有的没加？

这就要结合「谁在调它」来看了。

#### 1. 必须是 async def 的：
##### （1）ASGI 入口函数：mcp_asgi_app(scope, receive, send)

ASGI（FastAPI/Starlette 用的协议）规定：

- Web 请求处理函数签名：
    ```py
    async def app(scope, receive, send): ...
    ```
- 所以你的：
    ```py
    async def mcp_asgi_app(scope, receive, send):
        ...
    ```

必须是 `async def`，否则框架没法 `await` 它

##### （2）需要 await 的所有函数

例如：
```py
async def get_oai_tools_from_mcp(session: mcp.ClientSession) -> List[Dict[str, Any]]:
    tools_result = await session.list_tools()
    ...
```
- session.list_tools() 是一个异步操作（网络请求），只能在 async def 里 await；
- 所以这个函数本身就必须变成协程函数。

再比如：
```py
async def call_mcp_tool(...):
    result = await session.call_tool(...)
    ...
```

同理。

##### （3）chat_loop / run_agent_once 这种“流程控制函数”

chat_loop 里用了：
```py
async with websocket_client(ws_url) as (...):
    async with mcp.ClientSession(...) as mcp_session:
        await mcp_session.initialize()
        ...
        await run_agent_once(...)
```
- 里面大量用到了 `await、async with`；
- 所以 `chat_loop` 必须是 `async def`。

`run_agent_once` 被 `await` 调用，也是协程函数。

#### 2. 不需要 async 的函数（普通同步函数）

比如：
```py
def call_llm_with_tools(history, tools) -> Dict[str, Any]:
    response = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        tools=tools,
        tool_choice="auto",
        stream=False,
    )
    return response.choices[0].message
```
- 这里用的是 `llm_client.chat.completions.create(...)` 同步接口；
- 它本身会阻塞当前线程直到请求返回；
- 在你的脚本里，这个阻塞是可以接受的（没有特别追求 1 秒内同时处理几千用户那种规模）；
- 所以这个函数可以是普通的 def。

类似的还有：
```py
def final_summarize(history: List[Dict[str, Any]]) -> str:
    ...
    stream = llm_client.chat.completions.create(..., stream=True)
    for chunk in stream:
        ...
```

这个流式接口也是同步风格的迭代器，所以整个函数可以写成同步的。

#### 总结
>只要一个函数内部没有 `await`，也不需要作为 `ASGI / asyncio` 的回调，就完全可以写成普通 `def`