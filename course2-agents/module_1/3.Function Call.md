## Function Calling

### 方式 A

OpenAI / Qwen3 / 大部分主流 API 的 “官方 function calling” 标准接口

>Example
```py
response = client.chat.completions.create(
    model="qwen3-max",
    messages=[...],
    functions=[ {...schema...} ]
)
```

##### 🌟 1. 这是“官方协议层面的 function calling”

也就是：
- 模型内部经过专门训练来识别 JSON Schema
- 模型知道：
    - 何时调用函数
    - 以什么结构输出
    - JSON 参数如何填
    - 使用固定字段：function_call / arguments
- API 提供器（OpenAI、Qwen、Gemini、Claude）统一支持

##### 🌟 2. 输出格式有严格规范

例如 OpenAI/Qwen 都会返回类似：
```py
{
  "role": "assistant",
  "content": null,
  "function_call": {
    "name": "get_weather",
    "arguments": "{\"city\": \"上海\"}"
  }
}
```

格式是 完全确定的，客户端不需要自己解析乱七八糟的内容。

##### 🌟 3. 稳定、可验证、可扩展

- 使用 JSON Schema 做参数校验
- 系统自动补全参数
- 能强制结构化（降低 prompt 风险）
- 错误率低，几乎不产生 invalid JSON
- 更方便 IDE 自动生成类型

##### 🌟 4. 适合生产环境

你可以随意加入多个函数、复杂参数、嵌套对象。

这是你应该优先使用的方式。

### 方式 B
模型层级的“模拟 function calling”（纯 prompt 协议）

```py
system_prompt = "请根据工具回复用户，工具如下："
system_info = {"role": "system", "content": system_prompt, "tools": tools}

response, history = model.chat(...)
if isinstance(response, dict):
    fun_name = response.get("name")
```

##### 🌟 1. 这是由 “模型应用层” 自己发明的一套协议

模型内部 没有 function calling 的训练。

function 结构是：

你把函数列表塞到 prompt 里

模型按你的提示词「模拟」输出：

{"name": "...", "parameters": {...}}


这是纯 prompt 驱动，与 API 完全无关。

🌟 2. 输出结构不是标准，完全靠 prompt 稳定

比如可能出现：

JSON 少括号

JSON 多引号

JSON 嵌套格式错误

输出多余文本

混入自然语言导致解析失败

需要写很多“清洗/重试”逻辑。

🌟 3. 没有官方的 tools/schema 支持

参数类型校验要自己写

无法强约束结构（除非加 very strong prompt）

多函数或复杂函数很容易让模型搞错

🌟 4. 更像“早期 ReAct 风格 tool calling”

它本质等同于：

给模型一堆工具，让它按提示词格式吐出工具调用 JSON。