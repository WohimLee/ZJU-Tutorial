# client_ws.py
import os
import json
import asyncio
from typing import Dict, Any, List

from openai import OpenAI

import mcp
from mcp.client.websocket import websocket_client

from dotenv import load_dotenv
load_dotenv("/Users/azen/Desktop/llm/ZJU-Tutorial/.env")

############################################
# 1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ˆQwenï¼‰
############################################

llm_client = OpenAI(
    api_key=os.getenv("ALIBABA_API_KEY"),
    base_url=os.getenv("ALIBABA_API_URL"),
)

MODEL_NAME = "qwen3-max"

############################################
# 2. System æç¤ºè¯
############################################

SYSTEM_MESSAGE = {
    "role": "system",
    "content": (
        "ä½ æ˜¯ä¸€ä¸ªä¼šä¸»åŠ¨ä½¿ç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ã€‚"
        "å½“ä½ éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€å¤©æ°”ç­‰æ—¶ï¼Œè¯·ä¼˜å…ˆè°ƒç”¨æä¾›çš„å·¥å…·ã€‚"
        "ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·ï¼Œç›´åˆ°æ‹¿åˆ°è¶³å¤Ÿä¿¡æ¯åï¼Œå†ç»™å‡ºä¸­æ–‡å›ç­”ã€‚"
    ),
}

############################################
# 3. ä» MCP è¯»å–å·¥å…·åˆ—è¡¨ï¼Œè½¬æ¢ä¸º OpenAI tools schema
############################################


async def get_oai_tools_from_mcp(session: mcp.ClientSession) -> List[Dict[str, Any]]:
    """
    ä» MCP server è·å–å·¥å…·åˆ—è¡¨ï¼Œå¹¶è½¬æ¢æˆ OpenAI Chat Completions ç”¨çš„ tools schemaã€‚
    """
    tools_result = await session.list_tools()
    mcp_tools = [tool.model_dump() for tool in tools_result.tools]

    oai_tools: List[Dict[str, Any]] = []
    for t in mcp_tools:
        input_schema = t.get("inputSchema") or {}

        oai_tools.append(
            {
                "type": "function",
                "function": {
                    "name": t["name"],
                    "description": t.get("description", ""),
                    "parameters": input_schema,
                },
            }
        )

    return oai_tools


async def call_mcp_tool(
    session: mcp.ClientSession, tool_name: str, tool_args: Dict[str, Any]
) -> str:
    """
    è°ƒç”¨ MCP toolï¼ŒæŠŠè¿”å›çš„å†…å®¹ï¼ˆTextContent ç­‰ï¼‰æ‹¼æˆä¸€ä¸ªå­—ç¬¦ä¸²ç»™ LLMã€‚
    """
    result = await session.call_tool(name=tool_name, arguments=tool_args)

    parts: List[str] = []
    for item in result.content:
        # åªå–æ–‡æœ¬å†…å®¹ï¼Œå…¶ä»–ç±»å‹ï¼ˆå›¾ç‰‡ã€èµ„æºï¼‰ç®€å• str() ä¸€ä¸‹
        text = getattr(item, "text", None)
        if text is not None:
            parts.append(text)
        else:
            parts.append(str(item))

    return "\n".join(parts)


############################################
# 4. LLM è°ƒç”¨å°è£…ï¼ˆå¸¦ toolsï¼‰
############################################


def call_llm_with_tools(
    history: List[Dict[str, Any]], tools: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    è°ƒä¸€æ¬¡ LLMï¼Œè®©å®ƒå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆtool_choice='auto'ï¼‰ã€‚
    è¿”å› ChatCompletionMessageï¼ˆdictï¼‰ã€‚
    """
    response = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        tools=tools,
        tool_choice="auto",
        stream=False,
    )
    return response.choices[0].message


def final_summarize(history: List[Dict[str, Any]]) -> str:
    """
    æœ€ç»ˆæ€»ç»“é˜¶æ®µï¼šç¦æ­¢ç»§ç»­è°ƒå·¥å…·ï¼Œç”¨æµå¼è¾“å‡ºæ‰“å°æœ€ç»ˆå›ç­”ã€‚
    """
    print("åŠ©æ‰‹ï¼š", end="", flush=True)

    stream = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        stream=True,
        tool_choice="none",  # æœ€ç»ˆé˜¶æ®µç¦æ­¢å·¥å…·è°ƒç”¨
    )

    full_content = ""
    for chunk in stream:
        delta = chunk.choices[0].delta
        content = getattr(delta, "content", None) or ""
        if content:
            print(content, end="", flush=True)
            full_content += content
    print()
    return full_content


async def run_agent_once(
    user_input: str,
    history: List[Dict[str, Any]],
    mcp_session: mcp.ClientSession,
    oai_tools: List[Dict[str, Any]],
    max_tool_rounds: int = 5,
) -> str:
    """
    é’ˆå¯¹ä¸€æ¬¡ç”¨æˆ·è¾“å…¥ï¼Œæ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„ agent æµç¨‹ï¼ˆåŸºäº MCP å·¥å…·ï¼‰ï¼š
    - å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š max_tool_rounds è½®ï¼‰
    - æœ€åä½¿ç”¨ stream=True åšè‡ªç„¶è¯­è¨€æ€»ç»“
    è¿”å›æœ€ç»ˆå®Œæ•´å›å¤å­—ç¬¦ä¸²ã€‚
    """
    # å…ˆæŠŠç”¨æˆ·è¾“å…¥ push åˆ° history
    history.append({"role": "user", "content": user_input})

    # å·¥å…·è°ƒç”¨å¾ªç¯
    for _ in range(max_tool_rounds):
        msg = call_llm_with_tools(history, tools=oai_tools)

        assistant_msg: Dict[str, Any] = {
            "role": "assistant",
            "content": msg.content,
        }
        if msg.tool_calls:
            assistant_msg["tool_calls"] = msg.tool_calls
        history.append(assistant_msg)

        # æ²¡æœ‰ tool_callsï¼Œè¯´æ˜æ¨¡å‹è®¤ä¸ºå·²ç»å¯ä»¥ç›´æ¥å›ç­”
        if not msg.tool_calls:
            break

        # æœ‰ tool_calls -> é€šè¿‡ MCP è°ƒç”¨çœŸå®å·¥å…·
        for tool_call in msg.tool_calls:
            tool_name = tool_call.function.name
            raw_args = tool_call.function.arguments or "{}"

            try:
                if isinstance(raw_args, str):
                    tool_args = json.loads(raw_args)
                else:
                    tool_args = raw_args
            except json.JSONDecodeError:
                tool_args = {}

            print("æ¨¡å‹è¦æ±‚è°ƒç”¨å‡½æ•°ï¼š", tool_name, "å‚æ•°ï¼š", tool_args)

            try:
                tool_result = await call_mcp_tool(
                    session=mcp_session, tool_name=tool_name, tool_args=tool_args
                )
            except Exception as e:
                tool_result = f"[è°ƒç”¨ MCP å·¥å…· {tool_name} å‡ºé”™: {e!r}]"

            # æŠŠå·¥å…·ç»“æœå¡å› messages
            history.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": tool_name,
                    "content": str(tool_result),
                }
            )

    # å·¥å…·é˜¶æ®µç»“æŸ -> åšæœ€ç»ˆè‡ªç„¶è¯­è¨€å›ç­”
    final_answer = final_summarize(history)
    history.append({"role": "assistant", "content": final_answer})
    return final_answer


############################################
# 5. å†å²è£å‰ª + ä¸»å¾ªç¯
############################################


def truncate_history(
    history: List[Dict[str, Any]], max_messages: int = 30
) -> List[Dict[str, Any]]:
    """
    ç®€å•çš„å†å²è£å‰ªï¼šåªä¿ç•™æœ€è¿‘ max_messages æ¡æ¶ˆæ¯ï¼ˆåŠ ä¸Š systemï¼‰ã€‚
    """
    msgs = [m for m in history if m["role"] != "system"]
    if len(msgs) <= max_messages:
        return history

    new_history = [SYSTEM_MESSAGE] + msgs[-max_messages:]
    return new_history


async def chat_loop():
    """
    é€šè¿‡ WebSocket è¿æ¥æœ¬åœ° MCP serverï¼ˆserver_ws.pyï¼‰ï¼Œå¹¶è¿›å…¥å¤šè½®å¯¹è¯ã€‚
    """
    ws_url = "ws://127.0.0.1:8000/mcp/"

    print(f"å‡†å¤‡é€šè¿‡ WebSocket è¿æ¥ MCP æœåŠ¡ï¼š{ws_url}")

    # å»ºç«‹ WebSocket è¿æ¥ï¼Œæ‹¿åˆ° read / write æµ
    async with websocket_client(ws_url) as (read_stream, write_stream):
        # åˆ›å»º MCP ClientSession
        async with mcp.ClientSession(read_stream, write_stream) as mcp_session:
            # åˆå§‹åŒ– MCP ä¼šè¯ï¼ˆåå•†ç‰ˆæœ¬/èƒ½åŠ›ï¼‰
            await mcp_session.initialize()

            # ä» MCP server è·å–å·¥å…·åˆ—è¡¨ï¼Œå¹¶è½¬æ¢æˆ OpenAI tools schema
            oai_tools = await get_oai_tools_from_mcp(mcp_session)

            history: List[Dict[str, Any]] = [SYSTEM_MESSAGE]

            print("å·²å¯åŠ¨ MCP WebSocket å·¥å…·å¢å¼º Agentï¼Œå¯¹è¯ä¸­è¾“å…¥ exit / é€€å‡º å³å¯ç»“æŸã€‚")

            while True:
                user_input = input("ç”¨æˆ·ï¼š").strip()
                if user_input.lower() in {"exit", "quit", "q", "é€€å‡º"}:
                    print("å†è§ ğŸ‘‹")
                    break

                history = truncate_history(history)

                try:
                    await run_agent_once(
                        user_input,
                        history,
                        mcp_session=mcp_session,
                        oai_tools=oai_tools,
                    )
                except Exception as e:
                    print(f"\n[è°ƒç”¨å‡ºé”™]: {e!r}\n")


def main():
    asyncio.run(chat_loop())


if __name__ == "__main__":
    main()
