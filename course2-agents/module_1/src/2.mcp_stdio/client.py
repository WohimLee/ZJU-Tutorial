import os
import sys
import json
import asyncio
from typing import Dict, Any, List

from openai import OpenAI

from mcp import ClientSession, StdioServerParameters, types as mcp_types
from mcp.client.stdio import stdio_client

from dotenv import load_dotenv
load_dotenv("/Users/azen/Desktop/llm/ZJU-Tutorial/.env")

############################################
# 1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
############################################

llm_client = OpenAI(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·æ”¹æˆï¼š
    # api_key="sk-xxx",
    # base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("ALIBABA_API_KEY"),
    base_url=os.getenv("ALIBABA_API_URL"),
)

MODEL_NAME = "qwen3-max"

############################################
# 2. System æç¤ºè¯ï¼ˆåŸºç¡€è§’è‰²è®¾å®šï¼‰
############################################

SYSTEM_MESSAGE = {
    "role": "system",
    "content": (
        "ä½ æ˜¯ä¸€ä¸ªä¼šä¸»åŠ¨ä½¿ç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ã€‚"
        "å½“ä½ éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€å¤©æ°”ç­‰æ—¶ï¼Œè¯·ä¼˜å…ˆè°ƒç”¨æä¾›çš„å·¥å…·ã€‚"
        "ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·ï¼Œç›´åˆ°æ‹¿åˆ°è¶³å¤Ÿä¿¡æ¯åï¼Œå†ç»™å‡ºä¸­æ–‡å›ç­”ã€‚"
    ),
}

############################################
# 3. åŸºäº MCP çš„å·¥å…·å‘ç° & è½¬æ¢
############################################


async def get_oai_tools_from_mcp(session: ClientSession) -> List[Dict[str, Any]]:
    """
    ä» MCP server è·å–å·¥å…·åˆ—è¡¨ï¼Œå¹¶è½¬æ¢æˆ OpenAI Chat Completions æ‰€éœ€çš„ tools schemaã€‚
    """
    tools_result = await session.list_tools()
    mcp_tools = [tool.model_dump() for tool in tools_result.tools]

    oai_tools: List[Dict[str, Any]] = []
    for t in mcp_tools:
        # FastMCP çš„ Tool å¯¹è±¡ä¸­ï¼Œschema å­—æ®µåä¸€èˆ¬ä¸º inputSchema
        input_schema = t.get("inputSchema") or {}

        oai_tools.append(
            {
                "type": "function",
                "function": {
                    "name": t["name"],
                    "description": t.get("description", ""),
                    # MCP çš„ inputSchema ä¸ OpenAI çš„ parameters å­—æ®µè¯­ä¹‰ä¸€è‡´
                    "parameters": input_schema,
                },
            }
        )

    return oai_tools


async def call_mcp_tool(
    session: ClientSession, tool_name: str, tool_args: Dict[str, Any]
) -> str:
    """
    è°ƒç”¨ MCP toolï¼Œå¹¶æŠŠè¿”å›çš„å†…å®¹æ‹¼æˆä¸€ä¸ªå­—ç¬¦ä¸²ç»™ LLM history ä½¿ç”¨ã€‚
    """
    result = await session.call_tool(name=tool_name, arguments=tool_args)

    # MCP è¿”å›çš„æ˜¯ä¸€ç»„ content blockï¼Œæˆ‘ä»¬åªéœ€è¦æŠŠ text æ‹¼èµ·æ¥å³å¯
    parts: List[str] = []
    for item in result.content:
        # TextContent ç±»å‹æœ‰ text å±æ€§
        text = getattr(item, "text", None)
        if text is not None:
            parts.append(text)
        else:
            parts.append(str(item))

    return "\n".join(parts)


############################################
# 4. LLM è°ƒç”¨ï¼ˆä½¿ç”¨ MCP toolsï¼‰
############################################


def call_llm_with_tools(
    history: List[Dict[str, Any]], tools: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    è°ƒä¸€æ¬¡ LLMï¼Œè®©å®ƒå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆtool_choice='auto'ï¼‰ã€‚
    è¿”å›çš„æ˜¯ message å¯¹è±¡ï¼ˆdictï¼‰â€”â€” ChatCompletionMessageã€‚
    """
    response = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        tools=tools,
        tool_choice="auto",
        stream=False,
    )
    return response.choices[0].message


def final_summarize(history: List[Dict[str, Any]]) -> str:
    """
    ä½¿ç”¨æµå¼è¾“å‡ºçš„æ–¹å¼ï¼Œåœ¨ç»ˆç«¯å®æ—¶æ‰“å°å¤§æ¨¡å‹æœ€ç»ˆå›å¤ã€‚
    æ³¨æ„è¿™é‡Œ tool_choice='none'ï¼Œä¸å†å…è®¸è°ƒç”¨å·¥å…·ã€‚
    """
    print("åŠ©æ‰‹ï¼š", end="", flush=True)

    stream = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        stream=True,
        tool_choice="none",  # æœ€ç»ˆé˜¶æ®µç¦æ­¢å·¥å…·è°ƒç”¨
    )

    full_content = ""
    for chunk in stream:
        delta = chunk.choices[0].delta
        content = getattr(delta, "content", None) or ""
        if content:
            print(content, end="", flush=True)
            full_content += content
    print()  # æ¢è¡Œ
    return full_content


async def run_agent_once(
    user_input: str,
    history: List[Dict[str, Any]],
    mcp_session: ClientSession,
    oai_tools: List[Dict[str, Any]],
    max_tool_rounds: int = 5,
) -> str:
    """
    é’ˆå¯¹ä¸€æ¬¡ç”¨æˆ·è¾“å…¥ï¼Œæ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„ agent æµç¨‹ï¼ˆåŸºäº MCP å·¥å…·ï¼‰ï¼š
    - å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š max_tool_rounds è½®ï¼‰
    - æœ€åä½¿ç”¨ stream=True åšè‡ªç„¶è¯­è¨€æ€»ç»“
    è¿”å›æœ€ç»ˆå®Œæ•´å›å¤å­—ç¬¦ä¸²ã€‚
    """

    # å…ˆæŠŠç”¨æˆ·è¾“å…¥ push åˆ° history
    history.append({"role": "user", "content": user_input})

    # å¤šè½®å·¥å…·è°ƒç”¨å¾ªç¯
    for _ in range(max_tool_rounds):
        msg = call_llm_with_tools(history, tools=oai_tools)

        # å…ˆæŠŠæœ¬è½® assistant æ¶ˆæ¯åŠ è¿›å†å²ï¼ˆåŒ…æ‹¬å¯èƒ½çš„ tool_callsï¼‰
        assistant_msg: Dict[str, Any] = {
            "role": "assistant",
            "content": msg.content,
        }
        if msg.tool_calls:
            assistant_msg["tool_calls"] = msg.tool_calls
        history.append(assistant_msg)

        # å¦‚æœæ²¡æœ‰ tool_callsï¼Œè¯´æ˜æ¨¡å‹è§‰å¾—è‡ªå·±å·²ç»å¯ä»¥ç›´æ¥å›ç­”
        if not msg.tool_calls:
            break

        # å¦åˆ™ï¼Œé€šè¿‡ MCP è°ƒç”¨æ¯ä¸ªå·¥å…·
        for tool_call in msg.tool_calls:
            tool_name = tool_call.function.name
            raw_args = tool_call.function.arguments or "{}"

            try:
                if isinstance(raw_args, str):
                    tool_args = json.loads(raw_args)
                else:
                    tool_args = raw_args
            except json.JSONDecodeError:
                tool_args = {}

            print("æ¨¡å‹è¦æ±‚è°ƒç”¨å‡½æ•°ï¼š", tool_name, "å‚æ•°ï¼š", tool_args)

            try:
                tool_result = await call_mcp_tool(
                    session=mcp_session, tool_name=tool_name, tool_args=tool_args
                )
            except Exception as e:
                tool_result = f"[è°ƒç”¨ MCP å·¥å…· {tool_name} å‡ºé”™: {e!r}]"

            # æŠŠå·¥å…·ç»“æœä½œä¸º role=tool æ¶ˆæ¯å¡å›å»
            history.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": tool_name,
                    "content": str(tool_result),
                }
            )

        # ç„¶åè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ï¼Œçœ‹æ¨¡å‹è¦ä¸è¦å†æ¬¡å‘èµ· tool_calls

    # å·¥å…·é˜¶æ®µç»“æŸåï¼Œåšä¸€æ¬¡â€œæœ€ç»ˆå›ç­”â€ï¼Œç¦æ­¢ç»§ç»­è°ƒç”¨å·¥å…·
    final_answer = final_summarize(history)
    # æŠŠæœ€ç»ˆè‡ªç„¶è¯­è¨€å›ç­”ä¹ŸåŠ å…¥å†å²ï¼ˆæ–¹ä¾¿å¤šè½®å¯¹è¯ï¼‰
    history.append({"role": "assistant", "content": final_answer})
    return final_answer


############################################
# 5. å†å²è£å‰ª & ä¸»å…¥å£
############################################


def truncate_history(
    history: List[Dict[str, Any]], max_messages: int = 30
) -> List[Dict[str, Any]]:
    """
    ç®€å•çš„å†å²è£å‰ªï¼šåªä¿ç•™æœ€è¿‘ max_messages æ¡æ¶ˆæ¯ï¼ˆå¤–åŠ  systemï¼‰ã€‚
    é˜²æ­¢é•¿æ—¶é—´å¯¹è¯å¯¼è‡´ä¸Šä¸‹æ–‡å¤ªé•¿ã€‚
    """
    msgs = [m for m in history if m["role"] != "system"]
    if len(msgs) <= max_messages:
        return history

    # ä¿ç•™ system + æœ€è¿‘ N æ¡é system æ¶ˆæ¯
    new_history = [SYSTEM_MESSAGE] + msgs[-max_messages:]
    return new_history


async def chat_loop():
    """
    å¯åŠ¨ MCP server å­è¿›ç¨‹ï¼ˆé€šè¿‡ stdioï¼‰ï¼Œå¹¶è¿›è¡Œå¤šè½®å¯¹è¯ã€‚
    """
    # é…ç½®é€šè¿‡ stdio è¿æ¥åˆ°æœ¬åœ°çš„ server.py
    server_params = StdioServerParameters(
        command=sys.executable,  # å½“å‰ Python è§£é‡Šå™¨
        args=[
            "server.py",  # ç¡®ä¿ä¸ server æ–‡ä»¶åä¸€è‡´
        ],
    )

    # å¯¹è¯çº§åˆ«çš„ historyï¼ˆåŒ…å« systemï¼‰
    history: List[Dict[str, Any]] = [SYSTEM_MESSAGE]

    print("å·²å¯åŠ¨ MCP å·¥å…·å¢å¼º Agentï¼Œå¯¹è¯ä¸­è¾“å…¥ exit / é€€å‡º å³å¯ç»“æŸã€‚")

    # å¯åŠ¨ MCP å®¢æˆ·ç«¯ï¼ˆå­è¿›ç¨‹æ–¹å¼ï¼‰
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as mcp_session:
            # åˆå§‹åŒ– MCP ä¼šè¯
            await mcp_session.initialize()

            # ä» MCP åŠ¨æ€è·å–å·¥å…·ï¼Œå¹¶è½¬æ¢ä¸º OpenAI tools schema
            oai_tools = await get_oai_tools_from_mcp(mcp_session)

            while True:
                user_input = input("ç”¨æˆ·ï¼š").strip()
                if user_input.lower() in {"exit", "quit", "q", "é€€å‡º"}:
                    print("å†è§ ğŸ‘‹")
                    break

                # æ¯è½®å‰åšä¸€ä¸‹å†å²æˆªæ–­
                history = truncate_history(history)

                try:
                    await run_agent_once(
                        user_input,
                        history,
                        mcp_session=mcp_session,
                        oai_tools=oai_tools,
                    )
                except Exception as e:
                    print(f"\n[è°ƒç”¨å‡ºé”™]: {e!r}\n")


def main():
    asyncio.run(chat_loop())


if __name__ == "__main__":
    main()
