import os
import json
from typing import Annotated, Dict, Any, List

import requests
from openai import OpenAI

############################################
# 1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
############################################

llm_client = OpenAI(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·æ”¹æˆï¼š
    # api_key="sk-xxx",
    # base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("ALIBABA_API_KEY"),
    base_url=os.getenv("ALIBABA_API_URL"),
)

MODEL_NAME = "qwen3-max"

############################################
# 2. å·¥å…·å‡½æ•°å®šä¹‰
############################################

def get_weather(city: Annotated[str, 'The name of the city to be queried', True]):
    """
    æŸ¥è¯¢æŒ‡å®šåŸå¸‚å¤©æ°”ã€‚è¿”å›å­—ç¬¦ä¸²åŒ–çš„ç»“æœã€‚
    """
    if not isinstance(city, str):
        raise TypeError("City name must be a string")

    key_selection = {
        "current_condition": ["temp_C", "FeelsLikeC", "humidity", "weatherDesc", "observation_time"],
    }

    try:
        resp = requests.get(f"https://wttr.in/{city}?format=j1", timeout=10)
        resp.raise_for_status()
        data = resp.json()

        ret: Dict[str, Any] = {}
        for k, fields in key_selection.items():
            if k in data and data[k]:
                ret[k] = {}
                for field in fields:
                    value = data[k][0].get(field)
                    # weatherDesc æ˜¯ä¸ª list
                    if field == "weatherDesc" and isinstance(value, list) and value:
                        value = value[0].get("value")
                    ret[k][field] = value

        return json.dumps(ret, ensure_ascii=False)
    except Exception as e:
        return f"Error encountered while fetching weather data: {e!r}"

def search_info_by_tavily(query):
    from tavily import TavilyClient
    client = TavilyClient(os.getenv("TAVILY_API_KEY"))
    response = client.search(
        query=query
    )
    return json.dumps(response, ensure_ascii=False)

############################################
# 3. å·¥å…·æ³¨å†Œè¡¨ & tools æè¿°ï¼ˆç»™æ¨¡å‹çœ‹çš„ schemaï¼‰
############################################

TOOL_REGISTRY = {
    "get_weather": get_weather,
    "search_info_by_tavily": search_info_by_tavily,
}

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "æŸ¥è¯¢æŸä¸ªåŸå¸‚çš„å¤©æ°”",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°ï¼Œä¾‹å¦‚ï¼šæ·±åœ³ã€åŒ—äº¬ã€Shanghai ç­‰",
                    }
                },
                "required": ["city"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "search_info_by_tavily",
            "description": "Tavily æœç´¢ APIï¼Œæœç´¢ç›¸å…³ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string", 
                        "description": "ç”¨æˆ·æƒ³è¦æœç´¢çš„å†…å®¹"}
                },
                "required": ["query"],
            },
        },
    }
]

############################################
# 4. System æç¤ºè¯ï¼ˆåŸºç¡€è§’è‰²è®¾å®šï¼‰
############################################

SYSTEM_MESSAGE = {
    "role": "system",
    "content": (
        "ä½ æ˜¯ä¸€ä¸ªä¼šä¸»åŠ¨ä½¿ç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ã€‚"
        "å½“ä½ éœ€è¦æŸ¥è¯¢å®æ—¶ä¿¡æ¯ã€å¤©æ°”ç­‰æ—¶ï¼Œè¯·ä¼˜å…ˆè°ƒç”¨æä¾›çš„å·¥å…·ã€‚"
        "ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·ï¼Œç›´åˆ°æ‹¿åˆ°è¶³å¤Ÿä¿¡æ¯åï¼Œå†ç»™å‡ºä¸­æ–‡å›ç­”ã€‚"
    ),
}

############################################
# 5. Agent ä¸»å¾ªç¯ï¼šè‡ªåŠ¨å¤šè½®å·¥å…·è°ƒç”¨
############################################

def call_llm_with_tools(history: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    è°ƒä¸€æ¬¡ LLMï¼Œè®©å®ƒå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆtool_choice='auto'ï¼‰ã€‚
    è¿”å›çš„æ˜¯ message å¯¹è±¡ï¼ˆdictï¼‰ã€‚
    """
    response = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        tools=tools,
        tool_choice="auto",
        stream=False,
    )
    return response.choices[0].message


def run_agent_once(
    user_input: str,
    history: List[Dict[str, Any]],
    max_tool_rounds: int = 5,
) -> str:
    """
    é’ˆå¯¹ä¸€æ¬¡ç”¨æˆ·è¾“å…¥ï¼Œæ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„ agent æµç¨‹ï¼š
    - å¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š max_tool_rounds è½®ï¼‰
    - æœ€åä½¿ç”¨ stream=True åšè‡ªç„¶è¯­è¨€æ€»ç»“
    è¿”å›æœ€ç»ˆå®Œæ•´å›å¤å­—ç¬¦ä¸²ã€‚
    """

    # å…ˆæŠŠç”¨æˆ·è¾“å…¥ push åˆ° history
    history.append({"role": "user", "content": user_input})

    # å¤šè½®å·¥å…·è°ƒç”¨å¾ªç¯
    for round_idx in range(max_tool_rounds):
        msg = call_llm_with_tools(history)

        # å…ˆæŠŠæœ¬è½® assistant æ¶ˆæ¯åŠ è¿›å†å²ï¼ˆåŒ…æ‹¬å¯èƒ½çš„ tool_callsï¼‰
        assistant_msg = {
            "role": "assistant",
            "content": msg.content,
        }
        if msg.tool_calls:
            assistant_msg["tool_calls"] = msg.tool_calls
        history.append(assistant_msg)

        # å¦‚æœæ²¡æœ‰ tool_callsï¼Œè¯´æ˜æ¨¡å‹è§‰å¾—è‡ªå·±å·²ç»å¯ä»¥ç›´æ¥å›ç­”ï¼Œ
        # è¿™é‡Œå°±ç»“æŸå·¥å…·å¾ªç¯ï¼Œè¿›å…¥æœ€ç»ˆæ€»ç»“é˜¶æ®µã€‚
        if not msg.tool_calls:
            break

        # å¦åˆ™ï¼Œæ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
        for tool_call in msg.tool_calls:
            tool_name = tool_call.function.name
            raw_args = tool_call.function.arguments or "{}"

            try:
                if isinstance(raw_args, str):
                    tool_args = json.loads(raw_args)
                else:
                    tool_args = raw_args
            except json.JSONDecodeError:
                tool_args = {}
            print("æ¨¡å‹è¦æ±‚è°ƒç”¨å‡½æ•°ï¼š", tool_name, "å‚æ•°ï¼š", tool_args)
            func = TOOL_REGISTRY.get(tool_name)
            if func is None:
                tool_result = f"[å·¥å…· {tool_name} æœªåœ¨æœ¬åœ°æ³¨å†Œ]"
            else:
                try:
                    tool_result = func(**tool_args)
                except Exception as e:
                    tool_result = f"[è°ƒç”¨å·¥å…· {tool_name} å‡ºé”™: {e!r}]"

            # æŠŠå·¥å…·ç»“æœä½œä¸º role=tool æ¶ˆæ¯å¡å›å»
            history.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": tool_name,
                    "content": str(tool_result),
                }
            )

        # ç„¶åç»§ç»­ä¸‹ä¸€è½® forï¼Œçœ‹æ¨¡å‹è¦ä¸è¦å†æ¬¡å‘èµ· tool_calls

    # å·¥å…·é˜¶æ®µç»“æŸåï¼Œåšä¸€æ¬¡â€œæœ€ç»ˆå›ç­”â€ï¼Œç¦æ­¢ç»§ç»­è°ƒç”¨å·¥å…·
    final_answer = final_summarize(history)
    # æŠŠæœ€ç»ˆè‡ªç„¶è¯­è¨€å›ç­”ä¹ŸåŠ å…¥å†å²ï¼ˆæ–¹ä¾¿å¤šè½®å¯¹è¯ï¼‰
    history.append({"role": "assistant", "content": final_answer})
    return final_answer


############################################
# 6. æœ€ç»ˆæ€»ç»“é˜¶æ®µï¼ˆæµå¼è¾“å‡ºï¼‰
############################################

def final_summarize(history: List[Dict[str, Any]]) -> str:
    """
    ä½¿ç”¨æµå¼è¾“å‡ºçš„æ–¹å¼ï¼Œåœ¨ç»ˆç«¯å®æ—¶æ‰“å°å¤§æ¨¡å‹æœ€ç»ˆå›å¤ã€‚
    åŒæ—¶è¿”å›å®Œæ•´çš„å›å¤å­—ç¬¦ä¸²ã€‚
    æ³¨æ„è¿™é‡Œ tool_choice='none'ï¼Œä¸å†å…è®¸è°ƒç”¨å·¥å…·ã€‚
    """
    print("åŠ©æ‰‹ï¼š", end="", flush=True)

    stream = llm_client.chat.completions.create(
        model=MODEL_NAME,
        messages=history,
        stream=True,
        tool_choice="none",  # æœ€ç»ˆé˜¶æ®µç¦æ­¢å·¥å…·è°ƒç”¨
        # ä¹Ÿå¯ä»¥ä¸å†ä¼  toolsï¼›è¿™é‡Œä¼ ä¸ä¼ éƒ½è¡Œ
        # tools=tools,
    )

    full_content = ""
    for chunk in stream:
        delta = chunk.choices[0].delta
        content = getattr(delta, "content", None) or ""
        if content:
            print(content, end="", flush=True)
            full_content += content
    print()  # æ¢è¡Œ
    return full_content


############################################
# 7. ç®€å•çš„å¯¹è¯å…¥å£
############################################

def truncate_history(history: List[Dict[str, Any]], max_messages: int = 30) -> List[Dict[str, Any]]:
    """
    ç®€å•çš„å†å²è£å‰ªï¼šåªä¿ç•™æœ€è¿‘ max_messages æ¡æ¶ˆæ¯ï¼ˆå¤–åŠ  systemï¼‰ã€‚
    é˜²æ­¢é•¿æ—¶é—´å¯¹è¯å¯¼è‡´ä¸Šä¸‹æ–‡å¤ªé•¿ã€‚
    """
    msgs = [m for m in history if m["role"] != "system"]
    if len(msgs) <= max_messages:
        return history

    # ä¿ç•™ system + æœ€è¿‘ N æ¡é system æ¶ˆæ¯
    new_history = [SYSTEM_MESSAGE] + msgs[-max_messages:]
    return new_history


def main():
    # å¯¹è¯çº§åˆ«çš„ historyï¼ˆåŒ…å« systemï¼‰
    history: List[Dict[str, Any]] = [SYSTEM_MESSAGE]

    print("å·²å¯åŠ¨å·¥å…·å¢å¼º Agentï¼Œå¯¹è¯ä¸­è¾“å…¥ exit / é€€å‡º å³å¯ç»“æŸã€‚")
    while True:
        user_input = input("ç”¨æˆ·ï¼š").strip()
        if user_input.lower() in {"exit", "quit", "q", "é€€å‡º"}:
            print("å†è§ ğŸ‘‹")
            break

        # æ¯è½®å‰ç®€å•åšä¸€ä¸‹å†å²æˆªæ–­
        history = truncate_history(history)

        try:
            _ = run_agent_once(user_input, history)
        except Exception as e:
            print(f"\n[è°ƒç”¨å‡ºé”™]: {e!r}\n")


if __name__ == "__main__":
    main()
